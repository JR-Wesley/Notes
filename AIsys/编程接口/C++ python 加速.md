---
dateCreated: 2025-07-31
dateModified: 2025-07-31
---
# é«˜æ€§èƒ½è®¡ç®—ã€æ¨¡å‹éƒ¨ç½²ã€è‡ªå®šä¹‰ç®—å­å¼€å‘

## ğŸ§± ä¸€ã€ä½ éœ€è¦æŒæ¡çš„æ ¸å¿ƒçŸ¥è¯†ä½“ç³»

### 1. **C++14/17 åŸºç¡€ï¼ˆå¿…é¡»ï¼‰**

LibTorch æ˜¯ C++ åº“ï¼Œä½ éœ€è¦æ‰å®çš„ç°ä»£ C++ åŸºç¡€ã€‚

- âœ… åŸºç¡€è¯­æ³•ï¼šå˜é‡ã€å¾ªç¯ã€å‡½æ•°ã€æŒ‡é’ˆã€å¼•ç”¨
- âœ… é¢å‘å¯¹è±¡ï¼šç±»ã€ç»§æ‰¿ã€å¤šæ€
- âœ… æ¨¡æ¿ï¼ˆTemplatesï¼‰ï¼šå‡½æ•°æ¨¡æ¿ã€ç±»æ¨¡æ¿ï¼ˆéå¸¸é‡è¦ï¼ï¼‰
- âœ… STLï¼š`vector`, `string`, `memory`ï¼ˆæ™ºèƒ½æŒ‡é’ˆ `shared_ptr`, `unique_ptr`ï¼‰
- âœ… C++11/14/17 ç‰¹æ€§ï¼š`auto`, `lambda`, `move semantics`, `rvalue references`

---

### 2. **PyTorch çš„ Python APIï¼ˆç†Ÿæ‚‰ï¼‰**

ä½ æœ€ç»ˆæ˜¯è¦ç”¨ Python è°ƒç”¨ C++ï¼Œæ‰€ä»¥å¿…é¡»ç†è§£ PyTorch çš„è¯­ä¹‰ã€‚

- âœ… `torch.Tensor` çš„åˆ›å»ºã€æ“ä½œã€è®¾å¤‡ï¼ˆCPU/GPUï¼‰
- âœ… `torch.nn.Module`, `forward` æ–¹æ³•
- âœ… `torch.jit.trace`, `torch.jit.script`ï¼ˆç”¨äºæ¨¡å‹åºåˆ—åŒ–ï¼‰
- âœ… `torch.utils.cpp_extension` çš„ä½¿ç”¨ï¼ˆå¦‚ `load_inline`, `CUDAExtension`ï¼‰

---

### 3. **LibTorch C++ APIï¼ˆæ ¸å¿ƒï¼‰**

è¿™æ˜¯ä½ è¦é‡ç‚¹å­¦ä¹ çš„éƒ¨åˆ†ã€‚

| æ¨¡å— | å†…å®¹ |
|------|------|
| `#include <torch/torch.h>` | ä¸»å¤´æ–‡ä»¶ |
| `torch::Tensor` | C++ ä¸­çš„å¼ é‡ç±»å‹ï¼Œå¯¹åº” Python çš„ `torch.Tensor` |
| `torch::nn::Module` | å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å— |
| `torch::nn::Linear`, `torch::nn::Conv2d` ç­‰ | å¸¸è§å±‚ |
| `torch::optim::SGD`, `Adam` | ä¼˜åŒ–å™¨ |
| `torch::save()`, `torch::load()` | æ¨¡å‹ä¿å­˜ä¸åŠ è½½ |
| `torch::jit::load()` | åŠ è½½ Python å¯¼å‡ºçš„ `.pt` æ¨¡å‹ |

> ğŸ“š å®˜æ–¹æ–‡æ¡£ï¼š[LibTorch C++ API](https://pytorch.org/cppdocs/)

---

### 4. **å¦‚ä½•è®© C++ å‡½æ•°è¢« Python è°ƒç”¨ï¼Ÿ**

æœ‰ä¸¤ç§ä¸»æµæ–¹å¼ï¼š

#### âœ… æ–¹æ³• 1ï¼šä½¿ç”¨ `pybind11` + `torch::Tensor`ï¼ˆæ¨èï¼‰

- å°† C++ å‡½æ•°ç”¨ `pybind11` å°è£…ä¸º Python æ¨¡å—
- è¾“å…¥è¾“å‡ºä½¿ç”¨ `torch::Tensor`ï¼Œè‡ªåŠ¨ä¸ NumPy/PyTorch å…¼å®¹
- å¯ä»¥é…åˆ `torch.utils.cpp_extension.CUDAExtension` ç¼–è¯‘ CUDA ä»£ç 

```cpp
// binding.cpp
#include <pybind11/pybind11.h>
#include <torch/extension.h>

torch::Tensor add_tensors(torch::Tensor a, torch::Tensor b) {
    return a + b;
}

PYBIND11_MODULE(my_ops, m) {
    m.def("add_tensors", &add_tensors, "Add two tensors");
}
```

#### âœ… æ–¹æ³• 2ï¼šä½¿ç”¨ TorchScriptï¼ˆé€‚ç”¨äºæ¨¡å‹éƒ¨ç½²ï¼‰

- åœ¨ Python ä¸­ç”¨ `@torch.jit.script` æˆ– `torch.jit.trace` å¯¼å‡ºæ¨¡å‹ä¸º `.pt`
- åœ¨ C++ ä¸­ç”¨ `torch::jit::load("model.pt")` åŠ è½½å¹¶æ¨ç†

```cpp
auto module = torch::jit::load("model.pt");
auto output = module.forward({input_tensor});
```

> é€‚ç”¨åœºæ™¯ï¼š**éƒ¨ç½²è®­ç»ƒå¥½çš„æ¨¡å‹**ï¼Œè€Œä¸æ˜¯å¼€å‘æ–°ç®—å­ã€‚

---

### 5. **æ„å»ºç³»ç»Ÿï¼ˆBuild Systemï¼‰**

ä½ éœ€è¦å­¦ä¼šå¦‚ä½•ç¼–è¯‘ LibTorch é¡¹ç›®ã€‚

| å·¥å…· | è¯´æ˜ |
|------|------|
| `CMake` | æœ€å¸¸ç”¨ï¼ŒLibTorch å®˜æ–¹æ¨è |
| `setuptools` + `CUDAExtension` | é€‚åˆ Python æ‰©å±•ï¼Œè‡ªåŠ¨å¤„ç†ç¼–è¯‘ |
| `make` | ç®€å•é¡¹ç›®å¯ç”¨ï¼Œä½†ä¸æ¨èå¤æ‚é¡¹ç›® |

> ğŸ“š æ¨èï¼šä½¿ç”¨ `CMake` + `FindTorch.cmake`

---

## ğŸ§ª å››ã€ä¸€ä¸ªå®Œæ•´çš„å°é¡¹ç›®ç»ƒä¹ å»ºè®®

### ç›®æ ‡ï¼šå®ç°ä¸€ä¸ª `fused_relu` å‡½æ•°
- è¾“å…¥ï¼š`torch.Tensor`
- è¾“å‡ºï¼š`input.clamp(min=0)`ï¼ˆå³ ReLUï¼‰
- ç”¨ C++ å®ç°ï¼Œé€šè¿‡ `pybind11` æš´éœ²ç»™ Python

#### æ­¥éª¤
1. å†™ `relu_op.cpp`ï¼š

   ```cpp
   torch::Tensor fused_relu(torch::Tensor x) {
       return torch::clamp(x, 0);
   }
   ```

2. ç”¨ `pybind11` ç»‘å®š
3. ç”¨ `CUDAExtension` ç¼–è¯‘
4. Python ä¸­è°ƒç”¨å¹¶éªŒè¯ç»“æœ

---

## âœ… æ€»ç»“ï¼šä½ è¦å­¦ä»€ä¹ˆï¼Ÿ

| ç±»åˆ« | å†…å®¹ |
|------|------|
| **è¯­è¨€åŸºç¡€** | C++14/17ï¼ˆæ¨¡æ¿ã€æ™ºèƒ½æŒ‡é’ˆã€lambdaï¼‰|
| **æ ¸å¿ƒåº“** | LibTorch C++ APIï¼ˆ`torch::Tensor`, `torch::nn`ï¼‰|
| **ç»‘å®šæŠ€æœ¯** | `pybind11`ï¼ˆè®© C++ å‡½æ•°è¢« Python è°ƒç”¨ï¼‰|
| **æ„å»ºå·¥å…·** | `CMake` æˆ– `setuptools` + `CUDAExtension` |
| **éƒ¨ç½²æ–¹å¼** | TorchScriptï¼ˆæ¨¡å‹éƒ¨ç½²ï¼‰æˆ–è‡ªå®šä¹‰ç®—å­ï¼ˆæ‰©å±•ï¼‰|

---

## ğŸ’¡ å°è´´å£«

- ä» **CPU ç‰ˆæœ¬å¼€å§‹**ï¼Œå†æ‰©å±•åˆ° CUDA
- ä½¿ç”¨ `torch::without_grad()` å‡å°‘å†…å­˜å¼€é”€
- æ‰“å° tensor ç”¨ `std::cout << tensor << std::endl;`
- è°ƒè¯•æ—¶ç”¨ `assert(tensor.defined())` æ£€æŸ¥ç©ºæŒ‡é’ˆ

---

# Cuda åŠ é€Ÿ

## ğŸ§± å®ç°ä¸€ä¸ªâ€œå¯è°ƒç”¨â€çš„ CUDA ç®—å­ï¼ˆåŸºç¡€ç‰ˆï¼‰

ç›®æ ‡ï¼šæŠŠä½ çš„ CUDA kernel å°è£…æˆ `torch.nn.Module` æˆ–å‡½æ•°ï¼Œèƒ½åœ¨ PyTorch æ¨¡å‹ä¸­è°ƒç”¨ã€‚

### é¡¹ç›®ç»“æ„

```
my_extension/
â”œâ”€â”€ my_op.cu          # CUDA kernel
â”œâ”€â”€ my_op.cpp         # C++ ç»‘å®šä»£ç 
â”œâ”€â”€ setup.py          # ç¼–è¯‘è„šæœ¬
â””â”€â”€ test.py           # æµ‹è¯•æ¨¡å‹è°ƒç”¨
```

---

### 1. `my_op.cu`ï¼ˆç¤ºä¾‹ï¼šFused ReLU + Scaleï¼‰

```cpp
// my_op.cu
#include <cuda_runtime.h>

__global__ void fused_relu_scale_kernel(float* input, float* output, int N, float alpha) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        output[idx] = fmaxf(input[idx], 0.0f) * alpha;
    }
}

extern "C" void launch_fused_relu_scale(float* d_input, float* d_output, int N, float alpha) {
    int block_size = 256;
    int grid_size = (N + block_size - 1) / block_size;
    fused_relu_scale_kernel<<<grid_size, block_size>>>(d_input, d_output, N, alpha);
    cudaDeviceSynchronize();
}
```

---

### 2. `my_op.cpp`ï¼ˆC++ ç»‘å®š + PyTorch é›†æˆï¼‰

```cpp
// my_op.cpp
#include <torch/extension.h>

void launch_fused_relu_scale(float* d_input, float* d_output, int N, float alpha);

torch::Tensor fused_relu_scale(torch::Tensor input, float alpha) {
    auto output = torch::empty_like(input);
    launch_fused_relu_scale(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        input.numel(),
        alpha
    );
    return output;
}

// æ³¨å†Œä¸º PyTorch å‡½æ•°
PYBIND11_MODULE(my_ops, m) {
    m.def("fused_relu_scale", &fused_relu_scale, "Fused ReLU + Scale");
}
```

---

### 3. `setup.py`

```python
# setup.py
from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CUDAExtension

setup(
    name='my_ops',
    ext_modules=[
        CUDAExtension('my_ops', [
            'my_op.cu',
            'my_op.cpp',
        ]),
    ],
    cmdclass={'build_ext': BuildExtension}
)
```

---

### 4. ç¼–è¯‘

```bash
python setup.py build_ext --inplace
```

---

### 5. `test.py`ï¼ˆåœ¨ PyTorch æ¨¡å‹ä¸­ä½¿ç”¨ï¼‰

```python
import torch
from my_ops import fused_relu_scale

class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(100, 100)

    def forward(self, x):
        x = self.linear(x)
        x = fused_relu_scale(x, alpha=1.5)  # è°ƒç”¨ä½ çš„ CUDA ç®—å­
        return x

# æµ‹è¯•
model = MyModel().cuda()
x = torch.randn(32, 100, device='cuda')
y = model(x)
print(y.shape)
```

---

## âš¡ é˜¶æ®µ 3ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆæ¯” PyTorch åŸç”Ÿæ›´å¿«ï¼‰

ä½ ç°åœ¨èƒ½â€œè°ƒç”¨â€äº†ï¼Œä¸‹ä¸€æ­¥æ˜¯â€œæ›´å¿«â€ã€‚

### âœ… ä¼˜åŒ–æ–¹å‘

| ä¼˜åŒ–ç‚¹ | æ–¹æ³• | å·¥å…·/æŠ€å·§ |
|--------|------|-----------|
| **å†…å­˜è®¿é—®ä¼˜åŒ–** | åˆå¹¶å¤šä¸ªæ“ä½œï¼ˆfused kernelï¼‰| æŠŠ `ReLU + Scale + Add` åˆå¹¶ |
| **å‡å°‘å†…å­˜æ‹·è´** | åŸåœ°æ“ä½œï¼ˆin-placeï¼‰| `input.clamp_min_(0)` â†’ ä½† CUDA ä¸­éœ€å°å¿ƒ |
| **æé«˜å¹¶è¡Œåº¦** | ä½¿ç”¨ Shared Memoryã€Coalesced Access | æ‰‹åŠ¨ç®¡ç† `__shared__` |
| **ä½¿ç”¨ Tensor Core** | FP 16 + 1688 MMA æŒ‡ä»¤ | `__half`, `wmma` APIï¼ˆVolta+ï¼‰|
| **å‡å°‘ launch å¼€é”€** | åˆå¹¶å° kernel | ç”¨ä¸€ä¸ª kernel åšå¤šä¸ªäº‹ |

---

### ğŸ”¥ ç¤ºä¾‹ï¼šFused Bias + GeLUï¼ˆæ¯” `torch.nn.Linear + GELU` æ›´å¿«ï¼‰

```cpp
__global__ void fused_linear_gelu(float* input, float* weight, float* bias, float* output, int B, int I, int O) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B * O) return;

    int b = idx / O;
    int o = idx % O;

    float sum = bias[o];
    for (int i = 0; i < I; i++) {
        sum += input[b * I + i] * weight[o * I + i];
    }

    // GELU è¿‘ä¼¼
    float x = sum;
    float gelu = 0.5f * x * (1.0f + tanhf(0.7978845608028654f * x * (1.0f + 0.044715f * x * x)));
    output[idx] = gelu;
}
```

> è¿™ä¸ª kernel æŠŠ **Linear + Bias + GELU** ä¸‰æ­¥èåˆï¼Œå‡å°‘å†…å­˜è¯»å†™æ¬¡æ•°ï¼Œé€Ÿåº¦å¯æå‡ 2-3 å€ã€‚

---

## ğŸ“ˆ é˜¶æ®µ 4ï¼šæ€§èƒ½å¯¹æ¯”ä¸éªŒè¯

### å†™ä¸€ä¸ª Benchmark è„šæœ¬

```python
import torch
import time
from my_ops import fused_linear_gelu

# åŸç”Ÿå®ç°
class NativeModel(torch.nn.Module):
    def __init__(self, I, O):
        super().__init__()
        self.linear = torch.nn.Linear(I, O)
        self.gelu = torch.nn.GELU()

    def forward(self, x):
        return self.gelu(self.linear(x))

# è‡ªå®šä¹‰å®ç°ï¼ˆå‡è®¾å·²å°è£…ï¼‰
def custom_forward(input, weight, bias):
    return fused_linear_gelu(input, weight, bias)

# æµ‹è¯•
B, I, O = 32, 768, 3072
x = torch.randn(B, I, device='cuda')
model = NativeModel(I, O).cuda()

# åŸç”Ÿ
torch.cuda.synchronize()
t0 = time.time()
for _ in range(100):
    y1 = model(x)
torch.cuda.synchronize()
t1 = time.time()

# è‡ªå®šä¹‰
t2 = time.time()
for _ in range(100):
    y2 = custom_forward(x, model.linear.weight, model.linear.bias)
torch.cuda.synchronize()
t3 = time.time()

print(f"Native: {(t1-t0)*1000:.2f} ms")
print(f"Custom: {(t3-t2)*1000:.2f} ms")
print(f"Speedup: {(t1-t0)/(t3-t2):.2f}x")
```

---

## ğŸ§  é˜¶æ®µ 5ï¼šè¿›é˜¶æŠ€å·§ï¼ˆçœŸæ­£è¶…è¶ŠåŸç”Ÿï¼‰

| æŠ€å·§ | è¯´æ˜ |
|------|------|
| **ä½¿ç”¨ CUTLASS / CTK** | NVIDIA å®˜æ–¹çš„çº¿æ€§ä»£æ•°åº“ï¼Œæ”¯æŒ Tensor Core |
| **ä½¿ç”¨ CUDA Graphs** | å‡å°‘ kernel launch å¼€é”€ï¼Œé€‚åˆå›ºå®šè®¡ç®—å›¾ |
| **Kernel Fusion è‡ªåŠ¨åŒ–** | å€Ÿé‰´ TorchDynamo + Inductor æ€è·¯ |
| **Memory Pool ä¼˜åŒ–** | ä½¿ç”¨ `cudaMallocAsync` / `cudaFreeAsync`ï¼ˆCUDA 11.2+ï¼‰|
| **Profile é©±åŠ¨ä¼˜åŒ–** | ç”¨ `nsight-systems` æˆ– `nvprof` æ‰¾ç“¶é¢ˆ |

---

## ğŸ“š æ¨èå­¦ä¹ èµ„æº

| èµ„æº                                                                                          | è¯´æ˜                         |     |
| ------------------------------------------------------------------------------------------- | -------------------------- | --- |
| [PyTorch C++ Extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html)         | å®˜æ–¹æ•™ç¨‹                       |     |
| [NVIDIA CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/) | CUDA æƒå¨æ–‡æ¡£                  |     |
| [CUTLASS](https://github.com/NVIDIA/cutlass)                                                | é«˜æ€§èƒ½ GEMM åº“                 |     |
| [FlashAttention](https://github.com/HazyResearch/flash-attention)                           | å®æˆ˜å‚è€ƒï¼ˆèåˆ Attention + IO ä¼˜åŒ–ï¼‰|     |
| [Triton](https://github.com/openai/triton)                                                  | å¯é€‰ï¼šç”¨ Python å†™é«˜æ€§èƒ½ kernel    |     |
| [PyTorch C++ API æ–‡æ¡£](https://pytorch.org/cppdocs/)                                          | å®˜æ–¹æ–‡æ¡£ï¼Œå¿…çœ‹                    |     |
| [pybind11 å®˜æ–¹æ–‡æ¡£](https://pybind11.readthedocs.io/)                                           | å­¦ä¼šå¦‚ä½•ç»‘å®š C++ å’Œ Python        |     |
| [LibTorch Examples](https://github.com/pytorch/examples/tree/master/cpp)                    | å®˜æ–¹ C++ ç¤ºä¾‹                  |     |
| [pytorch/cpp-demo](https://github.com/pytorch/cpp-demo)                                     | ç®€å•çš„ C++ æ¨ç† demo            |     |
| [torchani/cpp](https://github.com/aiqm/torchani/tree/master/cpp)                            | å®é™…é¡¹ç›®å‚è€ƒ                     |     |
|                                                                                             |                            |     |

---

## ğŸ’¡ å°å»ºè®®

- ä» **å°ç®—å­** å¼€å§‹ï¼šReLUã€GELUã€LayerNorm
- ç”¨ **`torch.allclose()`** éªŒè¯æ•°å€¼æ­£ç¡®æ€§
- ç”¨ **`torch.cuda.synchronize()`** å‡†ç¡®è®¡æ—¶
- å…³æ³¨ **memory bandwidth bound** vs **compute bound**

---

å¦‚æœä½ å‘Šè¯‰æˆ‘ä½ æƒ³ä¼˜åŒ–çš„å…·ä½“ç®—å­ï¼ˆæ¯”å¦‚ LayerNormã€Attentionã€Softmaxï¼‰ï¼Œæˆ‘å¯ä»¥ç»™ä½ å†™ä¸€ä¸ªå®Œæ•´çš„ **é«˜æ€§èƒ½ CUDA å®ç° + PyTorch é›†æˆ demo**ã€‚æ¬¢è¿ç»§ç»­æé—®ï¼


# é¡¹ç›®ç®¡ç†

### 1. **PyBind11**ï¼ˆé¦–é€‰æ¨èï¼‰

- **ä½œç”¨**ï¼šå°† C++ï¼ˆåŒ…æ‹¬ CUDA ä»£ç ï¼‰å°è£…æˆ Python å¯è°ƒç”¨çš„æ¨¡å—ã€‚
- **ä¼˜ç‚¹**ï¼š
    - è½»é‡çº§ï¼Œå¤´æ–‡ä»¶åº“ï¼Œæ— éœ€å¤æ‚æ„å»ºç³»ç»Ÿã€‚
    - æ”¯æŒ NumPy æ•°ç»„æ— ç¼ä¼ é€’ï¼ˆ`py::array_t<float>`ï¼‰ã€‚
    - ç¼–è¯‘åç”Ÿæˆ `.so` æ–‡ä»¶ï¼ˆLinuxï¼‰æˆ– `.pyd`ï¼ˆWindowsï¼‰ï¼Œå¯ç›´æ¥ `import`ã€‚
- **å®˜ç½‘**ï¼šhttps://github.com/pybind/pybind11

> âœ… ç‰¹åˆ«é€‚åˆå°è£… CUDA kernel å¹¶æš´éœ²ç»™ Pythonã€‚


ä½¿ç”¨ `CMakeLists.txt` ç®¡ç†ç¼–è¯‘æµç¨‹ï¼Œè‡ªåŠ¨è°ƒç”¨ `nvcc` ç¼–è¯‘ `.cu` æ–‡ä»¶ï¼Œå¹¶é“¾æ¥ç”Ÿæˆ Python æ‰©å±•æ¨¡å—ã€‚

å¦‚æœä½ ä½¿ç”¨ PyTorchï¼šå¼ºçƒˆæ¨è `torch.utils.cpp_extension`

è¿™æ˜¯ PyTorch å®˜æ–¹æä¾›çš„å·¥å…·ï¼Œå¯ä»¥**ç›´æ¥ç¼–è¯‘ CUDA + C++ ä»£ç å¹¶åŠ è½½ä¸º Python æ¨¡å—**ã€‚
# æŠ€æœ¯æ ˆ

- âœ… ä½¿ç”¨ **CUDA** ç¼–å†™ kernel
- âœ… ä½¿ç”¨ **LibTorch + PyBind11** å°è£… C++ æ¥å£
- âœ… éœ€è¦æ”¯æŒå¤šä¸ªç®—å­ï¼ˆå¦‚ matmulã€bitonic sortã€MoE ç­‰ï¼‰
- âœ… æœªæ¥ä¼šé¢‘ç¹å¢åˆ ç®—å­
- âœ… å¸Œæœ›ç¼–è¯‘æ–¹å¼**ç®€å•ã€çµæ´»ã€å¯ç»´æŠ¤**

---

## âœ… æœ€ç»ˆç»“è®ºï¼š**æ¨èä½¿ç”¨ `torch.utils.cpp_extension` è€Œä¸æ˜¯çº¯ CMake**

### ğŸ”š ä¸€å¥è¯æ€»ç»“ï¼š
> **ç”¨ `torch.utils.cpp_extension.load`ï¼ˆå¼€å‘é˜¶æ®µï¼‰ + `setup.py` + `CUDAExtension`ï¼ˆå‘å¸ƒé˜¶æ®µï¼‰**ï¼Œå®Œå…¨æ›¿ä»£æ‰‹åŠ¨ CMakeï¼Œæ›´ç®€å•ã€æ›´é›†æˆã€æ›´é€‚åˆ PyTorch ç”Ÿæ€ã€‚

---

## ğŸ¤” ä¸ºä»€ä¹ˆä¸ç”¨çº¯ CMakeï¼Ÿ

| ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|
| çµæ´»ã€å¼ºå¤§ã€å·¥ä¸šçº§æ„å»ºç³»ç»Ÿ | é…ç½®å¤æ‚ï¼Œéœ€æ‰‹åŠ¨å¤„ç†ï¼š |
| | - LibTorch è·¯å¾„æŸ¥æ‰¾ |
| | - CUDA ç¼–è¯‘å™¨ (`nvcc`) è®¾ç½® |
| | - PyBind11 ä¸ Python å¤´æ–‡ä»¶å¯¹æ¥ |
| | - ç”Ÿæˆ `.so` å¹¶ç¡®ä¿èƒ½ `import` |
| | - ä¸åŒå¹³å°å…¼å®¹æ€§ï¼ˆLinux/macOS/Windowsï¼‰ |

ğŸ‘‰ å¯¹äº **PyTorch + CUDA æ‰©å±•å¼€å‘**ï¼ŒCMake æ˜¯â€œæ€é¸¡ç”¨ç‰›åˆ€â€ï¼Œè€Œ `torch.utils.cpp_extension` æ˜¯â€œé‡èº«å®šåˆ¶â€ã€‚

---

## âœ… æ¨èæ–¹æ¡ˆï¼šåˆ†é˜¶æ®µä½¿ç”¨ `torch.utils.cpp_extension`

### ğŸ§ª é˜¶æ®µ 1ï¼šå¼€å‘è°ƒè¯•é˜¶æ®µ â†’ ä½¿ç”¨ `load()`ï¼ˆå³æ—¶ç¼–è¯‘ï¼‰

```python
# compile_dev.py
from torch.utils.cpp_extension import load
import os

# åŠ¨æ€åˆ—å‡ºæ‰€æœ‰ç®—å­ç›®å½•
op_sources = {
    'matmul': ['src/kernels/matmul.cu', 'src/bindings/matmul.cpp'],
    'bitonic_sort': ['src/kernels/bitonic_sort.cu', 'src/bindings/bitonic_sort.cpp'],
    'moe': ['src/kernels/moe.cu', 'src/bindings/moe.cpp'],
}

# åŠ¨æ€ç¼–è¯‘å¹¶åŠ è½½
compiled_ops = {}
for op_name, sources in op_sources.items():
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œä¾¿äºå¢åˆ 
    if all(os.path.exists(s) for s in sources):
        compiled_ops[op_name] = load(
            name=f"cuda_op_{op_name}",
            sources=sources,
            verbose=True,
            with_cuda=True,
            extra_include_paths=["src/utils"],  # å¦‚æœ‰å¤´æ–‡ä»¶
            extra_cflags=['-O3'],
            extra_cuda_cflags=['-O3', '--use_fast_math']
        )
        print(f"âœ… {op_name} åŠ è½½æˆåŠŸ")
```

âœ… **ä¼˜ç‚¹**ï¼š
- ä¿®æ”¹ä»£ç åï¼Œä¸‹æ¬¡è¿è¡Œè‡ªåŠ¨é‡æ–°ç¼–è¯‘
- æ— éœ€å®‰è£…ï¼Œ`import` å³ç”¨
- æ”¯æŒçƒ­é‡è½½ï¼ˆé€‚åˆ Jupyter/Notebookï¼‰
- å¢åˆ ç®—å­åªéœ€ä¿®æ”¹ `op_sources` å­—å…¸

ğŸ”§ ä½¿ç”¨ï¼š
```python
x = torch.randn(100, 100, device='cuda')
y = compiled_ops['matmul'].matmul_forward(x, x.T)
```

---

### ğŸ“¦ é˜¶æ®µ 2ï¼šç¨³å®šé›†æˆé˜¶æ®µ â†’ ä½¿ç”¨ `setup.py`ï¼ˆæ­£å¼å®‰è£…ï¼‰

```python
# setup.py
from setuptools import setup, find_packages
from torch.utils.cpp_extension import BuildExtension, CUDAExtension
import os

# è‡ªåŠ¨æ‰«æç®—å­
def find_cuda_extensions():
    extensions = []
    op_dir = 'src/bindings'
    if not os.path.exists(op_dir):
        return extensions

    for fname in os.listdir(op_dir):
        if fname.endswith('.cpp'):
            op_name = fname[:-4]  # remove .cpp
            cpp_file = f'src/bindings/{op_name}.cpp'
            cu_file = f'src/kernels/{op_name}.cu'
            sources = [cpp_file]
            if os.path.exists(cu_file):
                sources.append(cu_file)

            extensions.append(
                CUDAExtension(
                    name=f'cuda_ops.{op_name}',
                    sources=sources,
                    include_dirs=['src/utils'],
                    extra_compile_args={
                        'cxx': ['-O3'],
                        'nvcc': ['-O3', '--use_fast_math']
                    }
                )
            )
    return extensions

setup(
    name='cuda_ops',
    version='0.1.0',
    packages=find_packages(),
    ext_modules=find_cuda_extensions(),
    cmdclass={'build_ext': BuildExtension},
    zip_safe=False,
)
```

ğŸ“¦ å®‰è£…ï¼š
```bash
pip install -e .
```

âœ… ä½¿ç”¨ï¼š
```python
import cuda_ops.matmul
import cuda_ops.moe

y = cuda_ops.matmul.matmul_forward(x, w)
```

---

## âœ… å·¥ç¨‹ç»“æ„å»ºè®®ï¼ˆæ”¯æŒåŠ¨æ€å¢åˆ ï¼‰

```
cuda_ops_project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ kernels/
â”‚   â”‚   â”œâ”€â”€ matmul.cu        # æ–°å¢ï¼šå¤åˆ¶æ¨¡æ¿å³å¯
â”‚   â”‚   â”œâ”€â”€ bitonic_sort.cu
â”‚   â”‚   â””â”€â”€ moe.cu
â”‚   â”œâ”€â”€ bindings/
â”‚   â”‚   â”œâ”€â”€ matmul.cpp       # ç»‘å®šæ–‡ä»¶ï¼Œå‘½åä¸€è‡´
â”‚   â”‚   â”œâ”€â”€ bitonic_sort.cpp
â”‚   â”‚   â””â”€â”€ moe.cpp
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ cuda_helpers.h
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_matmul.py
â”‚   â””â”€â”€ â€¦
â”œâ”€â”€ benchmarks/
â”œâ”€â”€ compile_dev.py           # å¼€å‘æ—¶ç”¨ load()
â”œâ”€â”€ setup.py                 # å‘å¸ƒæ—¶ç”¨
â””â”€â”€ README.md
```

---

## âœ… å¢åˆ ç®—å­æœ‰å¤šç®€å•ï¼Ÿ

### â• æ–°å¢ä¸€ä¸ªç®—å­ `gelu_approx.cu`

1. åˆ›å»º `src/kernels/gelu_approx.cu`
2. åˆ›å»º `src/bindings/gelu_approx.cpp`
3. è¿è¡Œ `python compile_dev.py` â†’ è‡ªåŠ¨ç¼–è¯‘åŠ è½½ âœ…

### â– åˆ é™¤ `bitonic_sort`

1. åˆ é™¤ `src/kernels/bitonic_sort.cu`
2. åˆ é™¤ `src/bindings/bitonic_sort.cpp`
3. `compile_dev.py` å’Œ `setup.py` ä¼šè‡ªåŠ¨è·³è¿‡ï¼ˆæ–‡ä»¶ä¸å­˜åœ¨ï¼‰âœ…

---

## âœ… é«˜çº§æŠ€å·§

### 1. å…±äº«å¤´æ–‡ä»¶
åœ¨ `src/utils/common.h` ä¸­å®šä¹‰ï¼š
```cpp
#pragma once
#include <torch/extension.h>
#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be on GPU")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")
```

åœ¨ `.cpp` æ–‡ä»¶ä¸­ä½¿ç”¨ï¼š
```cpp
torch::Tensor my_op(torch::Tensor x) {
    CHECK_CUDA(x);
    CHECK_CONTIGUOUS(x);
    // â€¦
}
```

### 2. æ”¯æŒ `torch.compile`
ç¡®ä¿ä½ çš„å‡½æ•°æ˜¯ pure functionï¼Œå¯è¢« `torch.compile` è¯†åˆ«ï¼š
```cpp
// åœ¨ .cpp ä¸­é¿å…å…¨å±€çŠ¶æ€
// ä½¿ç”¨ TORCH_LIBRARY() å®šä¹‰è‡ªå®šä¹‰ç®—å­ï¼ˆé«˜çº§ç”¨æ³•ï¼‰
```

---

## âœ… æ€»ç»“ï¼šä½ åº”è¯¥æ€ä¹ˆåšï¼Ÿ

| åœºæ™¯ | æ¨èæ–¹å¼ | å‘½ä»¤ |
|------|----------|------|
| **å¼€å‘è°ƒè¯•** | `torch.utils.cpp_extension.load` | `python compile_dev.py` |
| **æ­£å¼é›†æˆ** | `setup.py` + `CUDAExtension` | `pip install -e .` |
| **å¢åˆ ç®—å­** | åªéœ€å¢åˆ  `.cu` å’Œ `.cpp` æ–‡ä»¶ | è‡ªåŠ¨è¯†åˆ« âœ… |
| **CMake** | âŒ ä¸æ¨èï¼ˆé™¤éå·²æœ‰å¤§å‹ C++ é¡¹ç›®ï¼‰ | - |

---

ğŸ¯ **æœ€ä½³å®è·µ**ï¼š
1. ç”¨ `load()` å¿«é€Ÿè¿­ä»£å¼€å‘
2. ç”¨ `setup.py` æ‰“åŒ…å‘å¸ƒ
3. æ–‡ä»¶åç»Ÿä¸€ï¼ˆå¦‚ `op_name.cu` + `op_name.cpp`ï¼‰
4. åˆ©ç”¨ Python è„šæœ¬è‡ªåŠ¨æ‰«æï¼Œé¿å…ç¡¬ç¼–ç 

å¦‚æœä½ å¸Œæœ›ï¼Œæˆ‘å¯ä»¥ä¸ºä½ ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æ¨¡æ¿ä»“åº“ï¼ˆåŒ…æ‹¬ `compile_dev.py` è‡ªåŠ¨ç”Ÿæˆè„šæœ¬ï¼‰ï¼Œä½ åªéœ€å¾€ `kernels/` å’Œ `bindings/` ä¸¢æ–‡ä»¶å°±èƒ½ç”¨ã€‚æ¬¢è¿ç»§ç»­æé—®ï¼