---
dateCreated: 2025-07-31
dateModified: 2025-07-31
---
# é«˜æ€§èƒ½è®¡ç®—ã€æ¨¡å‹éƒ¨ç½²ã€è‡ªå®šä¹‰ç®—å­å¼€å‘

## ğŸ§± ä¸€ã€ä½ éœ€è¦æŒæ¡çš„æ ¸å¿ƒçŸ¥è¯†ä½“ç³»

### 1. **C++14/17 åŸºç¡€ï¼ˆå¿…é¡»ï¼‰**

LibTorch æ˜¯ C++ åº“ï¼Œä½ éœ€è¦æ‰å®çš„ç°ä»£ C++ åŸºç¡€ã€‚

- âœ… åŸºç¡€è¯­æ³•ï¼šå˜é‡ã€å¾ªç¯ã€å‡½æ•°ã€æŒ‡é’ˆã€å¼•ç”¨
- âœ… é¢å‘å¯¹è±¡ï¼šç±»ã€ç»§æ‰¿ã€å¤šæ€
- âœ… æ¨¡æ¿ï¼ˆTemplatesï¼‰ï¼šå‡½æ•°æ¨¡æ¿ã€ç±»æ¨¡æ¿ï¼ˆéå¸¸é‡è¦ï¼ï¼‰
- âœ… STLï¼š`vector`, `string`, `memory`ï¼ˆæ™ºèƒ½æŒ‡é’ˆ `shared_ptr`, `unique_ptr`ï¼‰
- âœ… C++11/14/17 ç‰¹æ€§ï¼š`auto`, `lambda`, `move semantics`, `rvalue references`

---

### 2. **PyTorch çš„ Python APIï¼ˆç†Ÿæ‚‰ï¼‰**

ä½ æœ€ç»ˆæ˜¯è¦ç”¨ Python è°ƒç”¨ C++ï¼Œæ‰€ä»¥å¿…é¡»ç†è§£ PyTorch çš„è¯­ä¹‰ã€‚

- âœ… `torch.Tensor` çš„åˆ›å»ºã€æ“ä½œã€è®¾å¤‡ï¼ˆCPU/GPUï¼‰
- âœ… `torch.nn.Module`, `forward` æ–¹æ³•
- âœ… `torch.jit.trace`, `torch.jit.script`ï¼ˆç”¨äºæ¨¡å‹åºåˆ—åŒ–ï¼‰
- âœ… `torch.utils.cpp_extension` çš„ä½¿ç”¨ï¼ˆå¦‚ `load_inline`, `CUDAExtension`ï¼‰

---

### 3. **LibTorch C++ APIï¼ˆæ ¸å¿ƒï¼‰**

è¿™æ˜¯ä½ è¦é‡ç‚¹å­¦ä¹ çš„éƒ¨åˆ†ã€‚

| æ¨¡å— | å†…å®¹ |
|------|------|
| `#include <torch/torch.h>` | ä¸»å¤´æ–‡ä»¶ |
| `torch::Tensor` | C++ ä¸­çš„å¼ é‡ç±»å‹ï¼Œå¯¹åº” Python çš„ `torch.Tensor` |
| `torch::nn::Module` | å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å— |
| `torch::nn::Linear`, `torch::nn::Conv2d` ç­‰ | å¸¸è§å±‚ |
| `torch::optim::SGD`, `Adam` | ä¼˜åŒ–å™¨ |
| `torch::save()`, `torch::load()` | æ¨¡å‹ä¿å­˜ä¸åŠ è½½ |
| `torch::jit::load()` | åŠ è½½ Python å¯¼å‡ºçš„ `.pt` æ¨¡å‹ |

> ğŸ“š å®˜æ–¹æ–‡æ¡£ï¼š[LibTorch C++ API](https://pytorch.org/cppdocs/)

---

### 4. **å¦‚ä½•è®© C++ å‡½æ•°è¢« Python è°ƒç”¨ï¼Ÿ**

æœ‰ä¸¤ç§ä¸»æµæ–¹å¼ï¼š

#### âœ… æ–¹æ³• 1ï¼šä½¿ç”¨ `pybind11` + `torch::Tensor`ï¼ˆæ¨èï¼‰

- å°† C++ å‡½æ•°ç”¨ `pybind11` å°è£…ä¸º Python æ¨¡å—
- è¾“å…¥è¾“å‡ºä½¿ç”¨ `torch::Tensor`ï¼Œè‡ªåŠ¨ä¸ NumPy/PyTorch å…¼å®¹
- å¯ä»¥é…åˆ `torch.utils.cpp_extension.CUDAExtension` ç¼–è¯‘ CUDA ä»£ç 

```cpp
// binding.cpp
#include <pybind11/pybind11.h>
#include <torch/extension.h>

torch::Tensor add_tensors(torch::Tensor a, torch::Tensor b) {
    return a + b;
}

PYBIND11_MODULE(my_ops, m) {
    m.def("add_tensors", &add_tensors, "Add two tensors");
}
```

#### âœ… æ–¹æ³• 2ï¼šä½¿ç”¨ TorchScriptï¼ˆé€‚ç”¨äºæ¨¡å‹éƒ¨ç½²ï¼‰

- åœ¨ Python ä¸­ç”¨ `@torch.jit.script` æˆ– `torch.jit.trace` å¯¼å‡ºæ¨¡å‹ä¸º `.pt`
- åœ¨ C++ ä¸­ç”¨ `torch::jit::load("model.pt")` åŠ è½½å¹¶æ¨ç†

```cpp
auto module = torch::jit::load("model.pt");
auto output = module.forward({input_tensor});
```

> é€‚ç”¨åœºæ™¯ï¼š**éƒ¨ç½²è®­ç»ƒå¥½çš„æ¨¡å‹**ï¼Œè€Œä¸æ˜¯å¼€å‘æ–°ç®—å­ã€‚

---

### 5. **æ„å»ºç³»ç»Ÿï¼ˆBuild Systemï¼‰**

ä½ éœ€è¦å­¦ä¼šå¦‚ä½•ç¼–è¯‘ LibTorch é¡¹ç›®ã€‚

| å·¥å…· | è¯´æ˜ |
|------|------|
| `CMake` | æœ€å¸¸ç”¨ï¼ŒLibTorch å®˜æ–¹æ¨è |
| `setuptools` + `CUDAExtension` | é€‚åˆ Python æ‰©å±•ï¼Œè‡ªåŠ¨å¤„ç†ç¼–è¯‘ |
| `make` | ç®€å•é¡¹ç›®å¯ç”¨ï¼Œä½†ä¸æ¨èå¤æ‚é¡¹ç›® |

> ğŸ“š æ¨èï¼šä½¿ç”¨ `CMake` + `FindTorch.cmake`

---

## ğŸ§ª å››ã€ä¸€ä¸ªå®Œæ•´çš„å°é¡¹ç›®ç»ƒä¹ å»ºè®®

### ç›®æ ‡ï¼šå®ç°ä¸€ä¸ª `fused_relu` å‡½æ•°
- è¾“å…¥ï¼š`torch.Tensor`
- è¾“å‡ºï¼š`input.clamp(min=0)`ï¼ˆå³ ReLUï¼‰
- ç”¨ C++ å®ç°ï¼Œé€šè¿‡ `pybind11` æš´éœ²ç»™ Python

#### æ­¥éª¤
1. å†™ `relu_op.cpp`ï¼š

   ```cpp
   torch::Tensor fused_relu(torch::Tensor x) {
       return torch::clamp(x, 0);
   }
   ```

2. ç”¨ `pybind11` ç»‘å®š
3. ç”¨ `CUDAExtension` ç¼–è¯‘
4. Python ä¸­è°ƒç”¨å¹¶éªŒè¯ç»“æœ

---

## âœ… æ€»ç»“ï¼šä½ è¦å­¦ä»€ä¹ˆï¼Ÿ

| ç±»åˆ« | å†…å®¹ |
|------|------|
| **è¯­è¨€åŸºç¡€** | C++14/17ï¼ˆæ¨¡æ¿ã€æ™ºèƒ½æŒ‡é’ˆã€lambdaï¼‰|
| **æ ¸å¿ƒåº“** | LibTorch C++ APIï¼ˆ`torch::Tensor`, `torch::nn`ï¼‰|
| **ç»‘å®šæŠ€æœ¯** | `pybind11`ï¼ˆè®© C++ å‡½æ•°è¢« Python è°ƒç”¨ï¼‰|
| **æ„å»ºå·¥å…·** | `CMake` æˆ– `setuptools` + `CUDAExtension` |
| **éƒ¨ç½²æ–¹å¼** | TorchScriptï¼ˆæ¨¡å‹éƒ¨ç½²ï¼‰æˆ–è‡ªå®šä¹‰ç®—å­ï¼ˆæ‰©å±•ï¼‰|

---

## ğŸ’¡ å°è´´å£«

- ä» **CPU ç‰ˆæœ¬å¼€å§‹**ï¼Œå†æ‰©å±•åˆ° CUDA
- ä½¿ç”¨ `torch::without_grad()` å‡å°‘å†…å­˜å¼€é”€
- æ‰“å° tensor ç”¨ `std::cout << tensor << std::endl;`
- è°ƒè¯•æ—¶ç”¨ `assert(tensor.defined())` æ£€æŸ¥ç©ºæŒ‡é’ˆ

---

# Cuda åŠ é€Ÿ

## ğŸ§± å®ç°ä¸€ä¸ªâ€œå¯è°ƒç”¨â€çš„ CUDA ç®—å­ï¼ˆåŸºç¡€ç‰ˆï¼‰

ç›®æ ‡ï¼šæŠŠä½ çš„ CUDA kernel å°è£…æˆ `torch.nn.Module` æˆ–å‡½æ•°ï¼Œèƒ½åœ¨ PyTorch æ¨¡å‹ä¸­è°ƒç”¨ã€‚

### é¡¹ç›®ç»“æ„

```
my_extension/
â”œâ”€â”€ my_op.cu          # CUDA kernel
â”œâ”€â”€ my_op.cpp         # C++ ç»‘å®šä»£ç 
â”œâ”€â”€ setup.py          # ç¼–è¯‘è„šæœ¬
â””â”€â”€ test.py           # æµ‹è¯•æ¨¡å‹è°ƒç”¨
```

---

### 1. `my_op.cu`ï¼ˆç¤ºä¾‹ï¼šFused ReLU + Scaleï¼‰

```cpp
// my_op.cu
#include <cuda_runtime.h>

__global__ void fused_relu_scale_kernel(float* input, float* output, int N, float alpha) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        output[idx] = fmaxf(input[idx], 0.0f) * alpha;
    }
}

extern "C" void launch_fused_relu_scale(float* d_input, float* d_output, int N, float alpha) {
    int block_size = 256;
    int grid_size = (N + block_size - 1) / block_size;
    fused_relu_scale_kernel<<<grid_size, block_size>>>(d_input, d_output, N, alpha);
    cudaDeviceSynchronize();
}
```

---

### 2. `my_op.cpp`ï¼ˆC++ ç»‘å®š + PyTorch é›†æˆï¼‰

```cpp
// my_op.cpp
#include <torch/extension.h>

void launch_fused_relu_scale(float* d_input, float* d_output, int N, float alpha);

torch::Tensor fused_relu_scale(torch::Tensor input, float alpha) {
    auto output = torch::empty_like(input);
    launch_fused_relu_scale(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        input.numel(),
        alpha
    );
    return output;
}

// æ³¨å†Œä¸º PyTorch å‡½æ•°
PYBIND11_MODULE(my_ops, m) {
    m.def("fused_relu_scale", &fused_relu_scale, "Fused ReLU + Scale");
}
```

---

### 3. `setup.py`

```python
# setup.py
from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CUDAExtension

setup(
    name='my_ops',
    ext_modules=[
        CUDAExtension('my_ops', [
            'my_op.cu',
            'my_op.cpp',
        ]),
    ],
    cmdclass={'build_ext': BuildExtension}
)
```

---

### 4. ç¼–è¯‘

```bash
python setup.py build_ext --inplace
```

---

### 5. `test.py`ï¼ˆåœ¨ PyTorch æ¨¡å‹ä¸­ä½¿ç”¨ï¼‰

```python
import torch
from my_ops import fused_relu_scale

class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(100, 100)

    def forward(self, x):
        x = self.linear(x)
        x = fused_relu_scale(x, alpha=1.5)  # è°ƒç”¨ä½ çš„ CUDA ç®—å­
        return x

# æµ‹è¯•
model = MyModel().cuda()
x = torch.randn(32, 100, device='cuda')
y = model(x)
print(y.shape)
```

---

## âš¡ é˜¶æ®µ 3ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆæ¯” PyTorch åŸç”Ÿæ›´å¿«ï¼‰

ä½ ç°åœ¨èƒ½â€œè°ƒç”¨â€äº†ï¼Œä¸‹ä¸€æ­¥æ˜¯â€œæ›´å¿«â€ã€‚

### âœ… ä¼˜åŒ–æ–¹å‘

| ä¼˜åŒ–ç‚¹ | æ–¹æ³• | å·¥å…·/æŠ€å·§ |
|--------|------|-----------|
| **å†…å­˜è®¿é—®ä¼˜åŒ–** | åˆå¹¶å¤šä¸ªæ“ä½œï¼ˆfused kernelï¼‰| æŠŠ `ReLU + Scale + Add` åˆå¹¶ |
| **å‡å°‘å†…å­˜æ‹·è´** | åŸåœ°æ“ä½œï¼ˆin-placeï¼‰| `input.clamp_min_(0)` â†’ ä½† CUDA ä¸­éœ€å°å¿ƒ |
| **æé«˜å¹¶è¡Œåº¦** | ä½¿ç”¨ Shared Memoryã€Coalesced Access | æ‰‹åŠ¨ç®¡ç† `__shared__` |
| **ä½¿ç”¨ Tensor Core** | FP 16 + 1688 MMA æŒ‡ä»¤ | `__half`, `wmma` APIï¼ˆVolta+ï¼‰|
| **å‡å°‘ launch å¼€é”€** | åˆå¹¶å° kernel | ç”¨ä¸€ä¸ª kernel åšå¤šä¸ªäº‹ |

---

### ğŸ”¥ ç¤ºä¾‹ï¼šFused Bias + GeLUï¼ˆæ¯” `torch.nn.Linear + GELU` æ›´å¿«ï¼‰

```cpp
__global__ void fused_linear_gelu(float* input, float* weight, float* bias, float* output, int B, int I, int O) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B * O) return;

    int b = idx / O;
    int o = idx % O;

    float sum = bias[o];
    for (int i = 0; i < I; i++) {
        sum += input[b * I + i] * weight[o * I + i];
    }

    // GELU è¿‘ä¼¼
    float x = sum;
    float gelu = 0.5f * x * (1.0f + tanhf(0.7978845608028654f * x * (1.0f + 0.044715f * x * x)));
    output[idx] = gelu;
}
```

> è¿™ä¸ª kernel æŠŠ **Linear + Bias + GELU** ä¸‰æ­¥èåˆï¼Œå‡å°‘å†…å­˜è¯»å†™æ¬¡æ•°ï¼Œé€Ÿåº¦å¯æå‡ 2-3 å€ã€‚

---

## ğŸ“ˆ é˜¶æ®µ 4ï¼šæ€§èƒ½å¯¹æ¯”ä¸éªŒè¯

### å†™ä¸€ä¸ª Benchmark è„šæœ¬

```python
import torch
import time
from my_ops import fused_linear_gelu

# åŸç”Ÿå®ç°
class NativeModel(torch.nn.Module):
    def __init__(self, I, O):
        super().__init__()
        self.linear = torch.nn.Linear(I, O)
        self.gelu = torch.nn.GELU()

    def forward(self, x):
        return self.gelu(self.linear(x))

# è‡ªå®šä¹‰å®ç°ï¼ˆå‡è®¾å·²å°è£…ï¼‰
def custom_forward(input, weight, bias):
    return fused_linear_gelu(input, weight, bias)

# æµ‹è¯•
B, I, O = 32, 768, 3072
x = torch.randn(B, I, device='cuda')
model = NativeModel(I, O).cuda()

# åŸç”Ÿ
torch.cuda.synchronize()
t0 = time.time()
for _ in range(100):
    y1 = model(x)
torch.cuda.synchronize()
t1 = time.time()

# è‡ªå®šä¹‰
t2 = time.time()
for _ in range(100):
    y2 = custom_forward(x, model.linear.weight, model.linear.bias)
torch.cuda.synchronize()
t3 = time.time()

print(f"Native: {(t1-t0)*1000:.2f} ms")
print(f"Custom: {(t3-t2)*1000:.2f} ms")
print(f"Speedup: {(t1-t0)/(t3-t2):.2f}x")
```

---

## ğŸ§  é˜¶æ®µ 5ï¼šè¿›é˜¶æŠ€å·§ï¼ˆçœŸæ­£è¶…è¶ŠåŸç”Ÿï¼‰

| æŠ€å·§ | è¯´æ˜ |
|------|------|
| **ä½¿ç”¨ CUTLASS / CTK** | NVIDIA å®˜æ–¹çš„çº¿æ€§ä»£æ•°åº“ï¼Œæ”¯æŒ Tensor Core |
| **ä½¿ç”¨ CUDA Graphs** | å‡å°‘ kernel launch å¼€é”€ï¼Œé€‚åˆå›ºå®šè®¡ç®—å›¾ |
| **Kernel Fusion è‡ªåŠ¨åŒ–** | å€Ÿé‰´ TorchDynamo + Inductor æ€è·¯ |
| **Memory Pool ä¼˜åŒ–** | ä½¿ç”¨ `cudaMallocAsync` / `cudaFreeAsync`ï¼ˆCUDA 11.2+ï¼‰|
| **Profile é©±åŠ¨ä¼˜åŒ–** | ç”¨ `nsight-systems` æˆ– `nvprof` æ‰¾ç“¶é¢ˆ |

---

## ğŸ“š æ¨èå­¦ä¹ èµ„æº

| èµ„æº                                                                                          | è¯´æ˜                         |     |
| ------------------------------------------------------------------------------------------- | -------------------------- | --- |
| [PyTorch C++ Extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html)         | å®˜æ–¹æ•™ç¨‹                       |     |
| [NVIDIA CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/) | CUDA æƒå¨æ–‡æ¡£                  |     |
| [CUTLASS](https://github.com/NVIDIA/cutlass)                                                | é«˜æ€§èƒ½ GEMM åº“                 |     |
| [FlashAttention](https://github.com/HazyResearch/flash-attention)                           | å®æˆ˜å‚è€ƒï¼ˆèåˆ Attention + IO ä¼˜åŒ–ï¼‰|     |
| [Triton](https://github.com/openai/triton)                                                  | å¯é€‰ï¼šç”¨ Python å†™é«˜æ€§èƒ½ kernel    |     |
| [PyTorch C++ API æ–‡æ¡£](https://pytorch.org/cppdocs/)                                          | å®˜æ–¹æ–‡æ¡£ï¼Œå¿…çœ‹                    |     |
| [pybind11 å®˜æ–¹æ–‡æ¡£](https://pybind11.readthedocs.io/)                                           | å­¦ä¼šå¦‚ä½•ç»‘å®š C++ å’Œ Python        |     |
| [LibTorch Examples](https://github.com/pytorch/examples/tree/master/cpp)                    | å®˜æ–¹ C++ ç¤ºä¾‹                  |     |
| [pytorch/cpp-demo](https://github.com/pytorch/cpp-demo)                                     | ç®€å•çš„ C++ æ¨ç† demo            |     |
| [torchani/cpp](https://github.com/aiqm/torchani/tree/master/cpp)                            | å®é™…é¡¹ç›®å‚è€ƒ                     |     |
|                                                                                             |                            |     |

---

## ğŸ’¡ å°å»ºè®®

- ä» **å°ç®—å­** å¼€å§‹ï¼šReLUã€GELUã€LayerNorm
- ç”¨ **`torch.allclose()`** éªŒè¯æ•°å€¼æ­£ç¡®æ€§
- ç”¨ **`torch.cuda.synchronize()`** å‡†ç¡®è®¡æ—¶
- å…³æ³¨ **memory bandwidth bound** vs **compute bound**

---

å¦‚æœä½ å‘Šè¯‰æˆ‘ä½ æƒ³ä¼˜åŒ–çš„å…·ä½“ç®—å­ï¼ˆæ¯”å¦‚ LayerNormã€Attentionã€Softmaxï¼‰ï¼Œæˆ‘å¯ä»¥ç»™ä½ å†™ä¸€ä¸ªå®Œæ•´çš„ **é«˜æ€§èƒ½ CUDA å®ç° + PyTorch é›†æˆ demo**ã€‚æ¬¢è¿ç»§ç»­æé—®ï¼
