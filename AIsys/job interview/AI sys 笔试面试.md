---
dateCreated: 2025-07-08
dateModified: 2025-07-09
---


# AIsys

CNN 的 PE 的接口和电路结构

矩阵乘法电路的实现方式及性能估计

1. 近似计算精度的评估、近似计算电路具体实现
2. 项目深挖，需要共享屏幕画电路架构图

## 模型

transformer 和 attention 的问题好答，其实核心内容就是多头注意力，他就是一个全局的加权求和，给相当于给把新的词向量表示成原来的词向量的线性组合，这个线性组合的系数就是一个相关性系数

<a href="https://zhuanlan.zhihu.com/p/636270877">大模型训练</a>

instructGPT

简单介绍 MoE 大模型，需要的计算模块

transformer 结构及需要的计算模块

Attention 的输入的含义和获得方式

 Deepseek 的论文





一面

项目，算子开发，cuda

静态链接，动态链接

红黑树，具体带系数的时间复杂度

内存泄漏，怎么解决

模板特化，偏特化，模板实例化是在哪个阶段，模板怎么拒绝一个类型

智能指针，shared_ptr 是线程安全的吗？

多线程和多进程，应用场景

协程

进程间通信，应用场景

二面

python list 去重

python 装饰器，作用

python 内存管理

linux 查看文件大小，查看网络状态

https://cppguide.cn/

ailab 高性能算子 -1 面

项目拷打 1，重点和部署量化流程和 gridsample 算子的优化以及算子的底层定义

项目拷打 234，重点是模型结构拷打

介绍一下访存密集型算子

介绍一下 fp32fp16int8 是怎么存储的

介绍一下量化原理以及过程

介绍一下 gpu 和 cpu 的结构以及适合计算什

udp 和 tcp 协议适合什么各自的优缺点

linux 查找磁盘使用指令

算法：链表判断有环➕cuda 实现向量加

ailab2 面

项目拷打重点同上

介绍一下锁的概念

介绍一下交叉熵的理解

介绍一下 cuda 编程的并行性和并发性

介绍一下 gan 和 cvae

手撕一下 svm

手撕一下 transformer

沐曦集成—ai 系统架构

项目拷打重点同上

介绍一下 cuda 编程的并行性和并发性

介绍一下 c++ 编程的三大特性

介绍一下 map 和 unorderedmap 底层实现

介绍一下 new，malloc，智能指针

介绍一下 new，malloc 的底层原理

介绍一下 lambda

手撕一下 transformer

算法：两数之和还有一个是 hash 具体啥忘了

随着大模型与生成式 AI 的爆发式增长，AI 基础设施正面临前所未有的性能、规模与效率挑战。该岗位致力于培养构建下一代 AI 系统底座的领军人才，具备软硬协同、跨层优化的知识面和技术深度，支撑集团核心 AI 业务的训练推理提效、集群资源调度及异构算力协同优化，推动 AI 技术的边界突破。核心问题包括但不限于：1. 极致性能优化：探索算法、训推引擎和基础设施的 co-design 协同突破效率瓶颈，最大化算力、网络和存储等硬件性能。2. 高性能网络：负责设计、实现、维护 AI 和高性能计算所需要的高性能网络通信框架和大模型推理场景的性能优化，聚焦模型通信场景的能力建设，完善集合通信、点对点通信等通信方式与推理框架的联合方案设计，推动提升推理性能。3. 智能资源调度：针对大规模分布式的 LLM/多模态理解生成训练推理等新兴计算场景，优化多集群多地域的异构调度编排能力，实现分钟级模型分发、训推任务弹性伸缩等。4. 其他随着 AI 模型、训推范式、算力硬件等迭代演进而出现的 AI 系统优化工程挑战和业界难题。

职位要求

1. 分布式系统、计算机体系结构、编译优化或通信与计算协同设计方向的硕/博士研究生。2. 具备 AI 训推计算性能分析与优化的经验，能深入分析 AI 模型在 GPU 平台上的性能瓶颈，提出并实施优化方案。针对分布式训练和推理系统，进行性能调优，提升系统的吞吐量和效率。3. 熟悉业界常见的优化栈（cuda/rocm/cutlass/ck/triton 等），在高效的内存管理、通信优化（NvLink/Infiniband/RoCEv2 等）关键技术上有实操经验。4. 分布式系统研发经验是加分项：设计和实现高效的分布式训练和推理框架，解决大规模分布式系统中的通信、同步和负载均衡问题。探索新型的分布式架构，提升系统的可扩展性和容错性。5. 前沿技术研究：跟踪 AI Infra 领域的最新研究进展，探索新的硬件架构、算法和系统优化技术。发表高水平学术论文，参与国际顶级会议（如 ISCA、MICRO、OSDI、SOSP、ATC、NSDI 等）。

2,熟悉 [后向误差传播算法](https://zhida.zhihu.com/search?content_id=153124779&content_type=Answer&match_order=1&q=%E5%90%8E%E5%90%91%E8%AF%AF%E5%B7%AE%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95&zhida_source=entity)（BP），完成从标量求导到矩阵求导思维方式的转换，熟悉常见算子的梯度推导（矩阵乘，卷积，池化，Relu，如果会 batch normalization 就一步到位了）；

3，熟悉 [autograd](https://zhida.zhihu.com/search?content_id=153124779&content_type=Answer&match_order=1&q=autograd&zhida_source=entity) 的基本原理，能自己手撸一个最好；

4，熟悉 cuda 编程（举一反三），熟悉 cuda 高阶用法，event, stream, 异步/同步，会优化常见 cuda kernel, element-wise, reduce, broadcast，MatMul, conv, pooling 等；

5，熟悉 c++ 和 python, 对 c++ 高级用法感到舒服，各种模式，惯用法，模板；熟悉 vim, [gdb](https://zhida.zhihu.com/search?content_id=153124779&content_type=Answer&match_order=1&q=gdb&zhida_source=entity) 程序调试；

6，熟悉 socket, [RDMA编程](https://zhida.zhihu.com/search?content_id=153124779&content_type=Answer&match_order=1&q=RDMA%E7%BC%96%E7%A8%8B&zhida_source=entity)，熟悉常见 [collective operation](https://zhida.zhihu.com/search?content_id=153124779&content_type=Answer&match_order=1&q=collective+operation&zhida_source=entity) 代价分析，譬如 ring allreduce, tree allreduce 代价分析；

7，熟悉多线程编程，熟悉锁，条件变量，内核线程，用户级线程，对 actor, CSP(coroutine) 各种技术熟悉；

8，熟悉 [编译器基本原理](https://zhida.zhihu.com/search?content_id=153124779&content_type=Answer&match_order=1&q=%E7%BC%96%E8%AF%91%E5%99%A8%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86&zhida_source=entity)，parser 什么的不重要，主要是 [dataflow分析](https://zhida.zhihu.com/search?content_id=153124779&content_type=Answer&match_order=1&q=dataflow%E5%88%86%E6%9E%90&zhida_source=entity)，灵活运用；熟悉多重循环程序优化技巧，譬如 polyhedral 模型；

9，熟悉常见 [分布式系统原理](https://zhida.zhihu.com/search?content_id=153124779&content_type=Answer&match_order=1&q=%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86&zhida_source=entity)，mapreduce, spark, flink, tensorflow 等；

10，熟悉 [计算机体系机构](https://zhida.zhihu.com/search?content_id=153124779&content_type=Answer&match_order=1&q=%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E6%9C%BA%E6%9E%84&zhida_source=entity)，量化分析方法，Amdahl' Law, Roofline Model, 流水线分析（譬如 David Patterson 那本书）；

11，熟悉操作系统原理及常用系统诊断工具，譬如各种资源利用率分析；

12，programming language 原理，命令式编程，函数式编程，逻辑编程，入门书《程序的构造与解释》？

13，熟悉项目构建原理，compiler, assembler, linker，loader 之类，有一本书《程序员的自我修养》有比较全面覆盖。

https://ucbrise.github.io/cs294-ai-sys-fa19/
https://zhuanlan.zhihu.com/p/608318764?share_code=6kgRiae9U4sA&utm_psn=1926408052585793087

- 大模型推理引擎研发工程师
岗位职责： 负责天数大模型推理技术的探索与研发，提升大模型在天数 GPGPU 上的推理性能： 1. 负责大模型推理引擎的架构设计与实现； 2. 负责大模型 server 的架构设计与实现； 3. 负责大模型推理通信库的设计及算法实现； 4. 负责大模型相关算子的实现及推理性能的整体优化； 5. 调研并支持大模型前沿算法；任职资格： 编程基础（必须）： 1. 熟悉 Python/C++ 编程（11标准及以上），了解常用数据结构及设计模式； 2. 熟悉深度学习编程框架，能够使用PyTorch 构建大模型推理 pipeline 并对模型中的核心模块进行高效实现； 3. 熟悉 CUDA 编程，了解常见算子的实现及优化手段；大模型推理技术（至少满足两项）： 1. 有 vLLM、TGI、SGLang、TensorRT-LLM等大模型推理框架的使用或优化经验； 2. 了解FlashAttention、PagedAttention、MoE、Chunked Prefill 等大模型核心技术； 3. 了解常见的大模型量化算法（如AWQ、GPTQ、SmoothQuant 等）及量化算子的实现； 4. 了解大模型通信算子（如Allreduce 等）及计算通信 overlap 实现； 5. 有分离式部署（PD 分离）开发经验。


高性能计算工程师
- CUDA
- 加速库
- GPU

岗位职责： 1. 研发高质量的芯片基础软件栈，包括支撑大语言模型在内的深度学习，线性代数，科学计算，信号处理等方向的基础加速库； 2. 分析重要的性能指标，调优已有软件算法，并且对未来的软件进行优化和提升； 3. 和硬件/应用的同事协同工作，一起对大语言模型，CV模型，语音模型，推荐系统，科学计算和其他相关并行算法领域进行分析和优化； 4. 负责 AI模型推理/训练的端到端性能整体优化； 5. 实现AI/科学计算领域在GPGPU上的最新算法和最佳实践； 6. 负责包括分子动力学、气象、量子力学等科学计算领域最新AI科学算法和传统算法在GPGPU上的实现和优化。 任职资格： 必须： 1. 熟悉 C++编程，了解常用数据结构及算法； 2. 熟悉线性代数，科学计算，机器学习，深度学习，AI算法等一个或多个领域； 3. 熟悉 GPU 或 AI加速芯片的体系结构，有过在不同架构芯片上的算法调优经验； 4. 熟悉并行算法优化的基本技巧，有对算法性能进行分析和优化的经验； 5. 熟悉计算机体系结构，操作系统； 加分项： 1. GPU的编程和优化经验（e.g. CUDA or OpenCL）； 2. 有过机器学习方面的开发经验（尤其是深度学习），熟悉Tensorflow, Pytorch, TensorRT等框架/引擎的原理和使用方法； 3. 有过视觉/大语言模型的开发和调优经验，熟悉CNN/Transformer等网络的性能瓶颈和优化方向，能够把握相关领域的国际前沿发展趋势； 4. 有过数据科学，统计科学，图像处理，信号处理等方面的开发和优化经历； 5. 有软硬件协同优化、基于异构硬件的软件开发经验者优先； 6. 了解微分方程求解的基本流程以及在GPU上优化的经验。有分子动力学、气象、以及量子力学等某一科学计算领域AI模型或者传统计算模型的开发经验。

- Horovod

岗位职责： 1. 研发GPGPU集合通信库； 2. 定位和解决应用中的分布式通信问题； 3. 分析优化分布式计算中的单机内/多机间集合通信性能。任职要求： 1. 熟悉C/C++编程； 2. 熟悉分布式常用的集合通信操作，了解常用的集合通信库，如OpenMPI、Gloo、NCCL； 3. 熟悉网络通信、RDMA技术，了解ibverbs编程接口； 4. 熟悉分布式训练框架，如PyTorch、Horovod； 5. 了解GPU体系架构和CUDA编程者优先； 6. 有类NCCL通信库开发经验者优先。


参考： https://zhuanlan.zhihu.com/p/608318764?share_code=6kgRiae9U4sA&utm_psn=1926408052585793087
# 知识体系

- 计算机
1. 操作系统
2. C++、C 等编程语言
3. 软件架构设计和优化、性能分析和调优
4. 数据结构与算法
5. linux
6. Qemu 模拟器 gem 5 / gpgpusim 等仿真工具框架
7. 集合通信原语（如 AllReduce, AllGather）和底层原理，RDMA 等高速网络通信技术

- 体系结构
1. CPU 流水线、缓存、分支预测等微架构原理
2. GPU
3. 可编程芯片（NPU/TPU）架构
4. AI 加速器
5. QEMU/GEM5 仿真模拟器

- 深度学习与框架
1. Python
2. LLVM/MLIR/TVM
3. 深度学习模型和框架（如 TensorFlow/Pytorch/Megatron/DeepSpeed）
4. ONNX PTX SASS
5. 主流模型 NLP/CV 模型架构与算法，MLP、CNN、LSTM、MHA、MLA、MOE、NLP (Bert), LLM (Transformer), diffusers

- 分布式训练或 HPC (高性能计算)
1. 分布式并行策略及其挑战（如数据并行、模型并行、流水线并行、、张量并行、序列并行、Zero 冗余优化器等）、内核级优化（如算子融合、内存管理优化、通信优化）
2. 主流框架（如 PyTorch 生态下的框架）适配到新型硬件（如 GPGPU/NPU/加速卡）。
3. 主流分布式训练/推理框架（如 DeepSpeed, Megatron-LM, Colossal-AI, FSDP, vLLM, SGLang, Hugging Face Accelerate/Transformers 等）千亿参数级别大模型训练或实战经验、推理优化
4. 高效的内存管理、通信优化（NvLink/Infiniband/RoCEv2 等）
5. 分析性能瓶颈（如通信开销、计算效率、内存限制）和可能的精度问题

- 算法
1. 数值计算、线性代数相关算法有深刻的理解
2. 卷积、矩阵乘、矩阵分解、BatchNorm、flash attention 密集型算子优化

- 算子并行优化
1. 并行编程基础：有 CUDA/OpenCL/OpenMP
2. GPU 高性能算子开发与优化，工具 Nsight Systems compute, DLProf, PyTorch Profiler, TensorBoard
3. Triton、TVM、MLIR 等深度学习专用编译器或编译器组件、编译器技术（如 TVM, MLIR, LLVM）在深度学习的应用。
4. NCCL、NVSHMEM 或其他分布式计算相关，MPI 开发、RDMA

- 数字集成电路
1. 互联通信协议
2. 算法硬件实现
3. 电路面积、时序、功耗优化
4. 硬件测试验证流程和工具（vcs, verdi, verilator, etc.）
5. SOC 和 IP 的架构/微架构设计探索 + 性能模型建模，包含不限于核心并行计算处理器、NOC、Cache、MMU、Memory、ESL/RDMA、die-to-die、一致性协议、DMA、etc。
6. 3 D 堆叠，chiplet


要构建面向 AI 芯片（LLM）、高性能计算（HPC）、模型加速优化领域的系统知识和技能体系，需从**基础理论、核心技术、工具链、实践项目**四个维度层层递进，结合行业前沿动态形成闭环。以下是具体框架与实施路径：

### 一、理论基础：构建跨学科知识网络

#### 1. **计算机体系结构与并行计算**

- **核心内容**：
    - **硬件架构**：掌握冯・诺依曼架构与哈佛架构的差异，理解 CPU/GPU/NPU 的异构计算原理（如英伟达 GPU 的 SM 单元、昇腾 NPU 的达芬奇架构）23。
    - **并行计算**：深入理解 Amdahl 定律、Gustafson 定律，掌握多线程（OpenMP）、分布式（MPI）、向量化（SIMD）等并行模式4。
    - **存储层次**：熟悉多级缓存（L1/L2/L3）、高带宽内存（HBM）、片上存储（SRAM）的性能差异，掌握数据局部性优化策略。
- **学习资源**：
    - 书籍：《计算机体系结构：量化研究方法》《高性能计算：并行分布式设计》
    - 课程：中山大学 “超算习堂” 平台的《高性能计算编程》课程1213

#### 2. **深度学习与大模型原理**

- **核心内容**：
    - **模型结构**：掌握 Transformer 架构（自注意力机制、位置编码）、MoE（专家混合模型）、多模态模型（如 Flamingo）的计算特性815。
    - **训练机制**：理解分布式训练（数据并行 / 模型并行 / 流水并行）、混合精度训练（FP16/BF16/INT8）的原理与实现15。
    - **推理优化**：熟悉推理引擎（TensorRT/ONNX Runtime）的工作流程，掌握动态 batch、算子融合、内存优化等技术614。
- **学习资源**：
    - 论文：《Attention Is All You Need》《Scaling Laws for Neural Language Models》
    - 实战：基于 Hugging Face 的 LLaMA-2 微调与推理优化

#### 3. **数学与算法基础**

- **核心内容**：
    - **线性代数**：精通矩阵运算（矩阵乘法优化、低秩分解）、特征值分解在模型压缩中的应用。
    - **数值计算**：掌握浮点运算特性（如舍入误差、溢出），理解量化对数值稳定性的影响。
    - **算法设计**：熟悉贪心算法（剪枝策略）、动态规划（算子调度）在模型优化中的应用7。
- **学习资源**：
    - 书籍：《线性代数及其应用》《数值分析》
    - 工具：MATLAB/Python 的 NumPy 库实践矩阵运算优化

### 二、核心技术：软硬件协同的关键能力

#### 1. **AI 芯片架构与编程**

- **核心技能**：
    - **架构设计**：掌握张量计算单元（如 Tensor Core）、片上网络（NoC）、Chiplet 设计的原理，理解华为昇腾、摩尔线程等国产芯片的架构差异。
    - **编程模型**：精通 CUDA（核函数优化、shared memory 使用）、OpenCL，熟悉国产芯片的编程框架（如昇腾 CANN、寒武纪 MLU-OPS）11。
    - **性能调优**：使用 Nsight Compute、HPCG 等工具分析计算瓶颈，优化访存带宽与计算密度。
- **实战路径**：
    - 复现英伟达 H100 的 FP8 矩阵乘法实现，对比昇腾 910B 的 BF16 性能差异9
    - 基于寒武纪思元芯片开发 Transformer 推理算子

#### 2. **高性能计算优化**

- **核心技能**：
    - **并行策略**：设计多级并行方案（节点间 MPI + 节点内 OpenMP+SIMD 向量化），解决负载不均衡问题。
    - **通信优化**：掌握 RDMA、NVLink 等高速互联技术，优化 AllReduce、Gather/Scatter 等集体通信操作。
    - **编译器优化**：使用 LLVM/MLIR 进行循环展开、自动向量化，理解算子融合的底层实现。
- **实战路径**：
    - 在超算集群上优化分子动力学模拟程序（如 NAMD）的并行效率
    - 基于 MLIR 实现矩阵乘法的自动分块与访存优化

#### 3. **模型加速与压缩技术**

- **核心技能**：
    - **量化技术**：掌握 PTQ（训练后量化）、QAT（量化感知训练）的原理，熟悉 INT8/FP8 混合精度部署。
    - **剪枝策略**：实现结构化剪枝（通道剪枝）与非结构化剪枝（权重稀疏化），结合硬件稀疏计算单元优化。
    - **模型蒸馏**：设计知识蒸馏方案（如教师 - 学生模型），优化端侧推理性能。
- **实战路径**：
    - 使用 TensorRT 对 BERT 模型进行 INT8 量化，对比精度与速度的平衡点
    - 基于 SepLLM 框架实现长文本推理的稀疏注意力优化


以下是面向 AI 芯片（LLM）、高性能计算（HPC）、模型加速优化领域的系统性知识和技能体系，结合 2025 年最新技术趋势与行业动态，分为**理论基础、核心技术、工具链、实践路径**四大模块，包含关键理论要点、工具推荐及前沿技术方向：

### 一、理论基础：构建跨学科知识网络

#### 1. **计算机体系结构与并行计算**

- **核心理论**：
    - **异构计算原理**：掌握 CPU/GPU/NPU 的架构差异（如英伟达 GPU 的 SM 单元、昇腾 NPU 的达芬奇架构），理解冯・诺依曼架构与哈佛架构的内存访问特性1。
    - **并行计算模型**：深入理解 Amdahl 定律、Gustafson 定律，掌握多线程（OpenMP）、分布式（MPI）、向量化（SIMD）的协同优化策略。
    - **Chiplet 设计**：学习 UCIe 标准与芯粒互连技术，理解分解式计算如何提升芯片能效比（如 Arm CSA 架构的模块化设计）5。
- **实践工具**：
    - 体系结构仿真：GEM5、QEMU（支持 Chiplet 建模）
    - 并行性能分析：Intel VTune、NVIDIA Nsight Compute

#### 2. **深度学习与大模型原理**

- **核心理论**：
    - **模型结构**：精通 Transformer 架构（自注意力机制、位置编码）、MoE（专家混合模型）的计算特性，理解动态稀疏激活（如 SparseGPT）对推理性能的影响。
    - **训练机制**：掌握分布式训练（数据并行 / 模型并行 / 流水并行）、混合精度训练（FP8/BF16/INT8）的原理，理解梯度同步与通信优化策略。
    - **推理优化**：熟悉推理引擎（TensorRT-LLM、vLLM）的工作流程，掌握动态 batch、算子融合、内存优化等技术8。
- **实践工具**：
    - 模型压缩：Hugging Face Optimum、TensorRT-LLM
    - 动态推理框架：Hugging Face TGI（支持连续批处理与 PagedAttention）

#### 3. **数学与算法基础**

- **核心理论**：
    - **线性代数**：精通矩阵乘法优化（如 Strassen 算法）、低秩分解在模型压缩中的应用，理解 FP8/BF16 的数值稳定性。
    - **数值计算**：掌握量化误差分析（如 KL 散度量化）、定点化技术（如 QAT 量化感知训练）。
    - **算法设计**：熟悉贪心算法（剪枝策略）、动态规划（算子调度）在模型优化中的应用。
- **实践工具**：
    - 数学建模：MATLAB、NumPy
    - 量化分析：PyTorch QAT 工具链、TensorFlow Lite Micro

### 二、核心技术：软硬件协同的关键能力

#### 1. **AI 芯片架构与编程**

- **核心技能**：
    - **架构设计**：掌握张量计算单元（如 Tensor Core）、片上网络（NoC）、Chiplet 设计的原理，理解华为昇腾、摩尔线程等国产芯片的架构差异。
    - **编程模型**：精通 CUDA（核函数优化、shared memory 使用）、OpenCL，熟悉国产芯片的编程框架（如昇腾 CANN、寒武纪 MLU-OPS）。
    - **性能调优**：使用 Nsight Compute、HPCG 等工具分析计算瓶颈，优化访存带宽与计算密度。
- **前沿技术**：
    - **全自动芯片设计**：学习中科院「启蒙」系统的 AI 驱动芯片设计流程，掌握硬件代码自动生成（CodeV 系列）与操作系统内核优化（AutoOS）2。
    - **异构计算优化**：基于 LLVM/MLIR 实现跨芯片算子适配（如 FlagGems 支持 180 + 算子）10。

#### 2. **高性能计算优化**

- **核心技能**：
    - **并行策略**：设计多级并行方案（节点间 MPI + 节点内 OpenMP+SIMD 向量化），解决负载不均衡问题。
    - **通信优化**：掌握 RDMA、NVLink 等高速互联技术，优化 AllReduce、Gather/Scatter 等集体通信操作。
    - **编译器优化**：使用 LLVM/MLIR 进行循环展开、自动向量化，理解算子融合的底层实现。
- **前沿技术**：
    - **云超算标准化**：学习 GB/T 45400-2025 国家标准，掌握弹性高性能计算（E-HPC V2.0）的资源调度与成本优化11。
    - **量子 - 经典混合计算**：探索量子自动学习（QAL）、张量网络建模在 HPC 中的应用7。

#### 3. **模型加速与压缩技术**

- **核心技能**：
    - **量化技术**：掌握 PTQ（训练后量化）、QAT（量化感知训练）的原理，熟悉 INT8/FP8 混合精度部署。
    - **剪枝策略**：实现结构化剪枝（通道剪枝）与非结构化剪枝（权重稀疏化），结合硬件稀疏计算单元优化。
    - **模型蒸馏**：设计知识蒸馏方案（如教师 - 学生模型），优化端侧推理性能。
- **前沿技术**：
    - **动态推理优化**：使用 Hugging Face TGI 的连续批处理与流式输出，提升长序列生成效率。
    - **自我奖励机制**：学习 LaTRO 框架的隐变量推理优化，通过自我评估提升多步骤推理准确率9。

### 三、工具链：从理论到工业级落地

#### 1. **开发工具与框架**

- **编译器与调试**：
    - **LLVM/MLIR**：实现跨芯片算子编译优化，支持昇腾、寒武纪等国产架构。
    - **Nsight Compute**：分析 GPU 计算瓶颈，优化矩阵运算效率。
- **推理引擎**：
    - **TensorRT-LLM**：支持千亿参数模型的 INT8 量化与多 GPU 并行推理。
    - **vLLM**：通过 PagedAttention 优化长序列生成的内存利用率。
- **混合精度工具**：
    - **Apex**：PyTorch 生态的混合精度训练库。
    - **BF16/FP8 原生支持**：利用英伟达 H100、昇腾 910B 的硬件加速特性。

#### 2. **开源项目与社区**

- **FlagGems**：跨芯片算子库，支持 PyTorch 生态，性能超厂商原生算子 30%。
- **启蒙系统**：全自动芯片设计工具，支持 RISC-V CPU 与操作系统内核优化。
- **SepLLM**：大模型稀疏注意力优化框架，支持动态 KV 缓存管理。

#### 3. **行业标准与认证**

- **认证体系**：
    - **NVIDIA 认证**：CUDA 认证工程师（CCE）、NVIDIA 深度学习学院（DLI）课程。
    - **国产认证**：华为 HCIA-AI、寒武纪开发者认证。
- **行业标准**：
    - **ONNX/OpenVINO**：模型转换与部署标准。
    - **UCIe 2.0**：Chiplet 互连协议，支持多芯粒协同设计。

### 四、实践路径：从理论到职业竞争力

#### 1. **分阶段学习计划**

- **阶段一：筑基（3-6 个月）**
    - 完成《高性能计算编程》课程，掌握 CUDA 核函数开发与 MPI 并行编程。
    - 复现经典模型（如 ResNet）的量化与剪枝实验，使用 TensorRT 进行 INT8 部署。
- **阶段二：精进（6-12 个月）**
    - 参与开源项目（如 FlagGems），开发跨芯片算子并提交代码贡献。
    - 在超算集群上优化大模型推理性能（如 LLaMA-2 70B），对比昇腾 910B 与英伟达 H100 的性能差异。
- **阶段三：实战（12 个月以上）**
    - 主导企业级项目（如金融风控模型的端侧部署），实现推理速度提升 2 倍以上。
    - 发表技术论文或开源工具（如自定义算子库），参与行业竞赛（如 ASC 超算竞赛）。

#### 2. **行业动态跟踪**

- **技术趋势**：
    - **Chiplet 设计**：关注 Intel 18A 工艺与 Arm CSA 架构的商业化进展4。
    - **量子 AI**：探索量子自动学习（QAL）在复杂推理任务中的应用。
- **政策与生态**：
    - **国产替代**：跟踪「十四五」集成电路政策，理解华为昇腾、寒武纪等厂商的技术路线6。
    - **开源生态**：参与 PyTorch 基金会项目（如 FlagGems），推动多芯片兼容性。

### 五、核心学习资源整合

|领域|书籍推荐|课程平台|开源项目|
|---|---|---|---|
|AI 芯片架构|《人工智能芯片设计》《升腾 AI 处理器架构与编程》|中山大学 “超算习堂”|FlagGems、启蒙系统|
|高性能计算|《高性能计算：并行分布式设计》|Coursera 的《HPC Programming》|OpenMPI、OpenMP|
|模型加速|《模型压缩技术实战》《TensorRT 实战：从模型转换到推理优化》|Udacity 的《深度学习推理优化》|TGI、LaTRO|
|工具链|《LLVM 核心开发指南》《MLIR 入门教程》|极客时间《编译器实战课》|MLIR、LLVM|

### 总结

构建这一领域的知识体系需遵循 “**理论筑基→工具精通→项目实战→行业落地**” 的路径，重点关注**软硬件协同优化**与**国产替代趋势**。建议通过开源项目积累差异化竞争力，同时保持对技术前沿的敏感度（如全自动芯片设计、量子 AI 融合）。在求职时，突出**量化成果**与**复杂问题解决能力**，选择技术壁垒高、生态合作广泛的企业，在国产替代浪潮中抢占先机。



# AI Sys

在 AI 芯片公司，大语言模型（LLM）从算法定义到最终在自研硬件上部署，需要经历**7 个核心系统层次**的协同设计，每个层次都需适配芯片特性（如计算单元架构、存储层次、互联方式），同时解决性能、能效和兼容性问题。以下是各层次的具体设计要点：

### **一、算法层：模型结构与训练目标定义**

- **核心任务**：确定 LLM 的基础架构（如 Transformer 变体）、参数量（如 7B/70B / 千亿级）、训练目标（如预训练 / 微调）和推理场景（如对话 / 生成）。
- **设计要点**：
    - 针对芯片计算特性调整模型结构（如若芯片支持稀疏计算，可设计稀疏注意力机制）；
    - 确定量化策略（如 FP16/INT8/FP8），平衡精度与算力利用率（自研芯片可能有专用低精度计算单元）；
    - 优化序列长度（如支持动态上下文窗口），适配芯片的片上存储容量（如 HBM 带宽）。

### **二、框架层：分布式训练 / 推理框架适配**

- **核心任务**：将 LLM 模型代码（如基于 PyTorch/TensorFlow）适配到自研芯片，通过分布式框架实现大规模训练 / 推理。
- **设计要点**：
    - **框架移植**：修改主流框架（如 Megatron-LM、vLLM）的硬件接口，将模型计算映射到芯片的计算核（如替换 CUDA 调用为芯片专用 API）；
    - **并行策略**：设计混合并行方案（数据并行 + 张量并行 + 流水线并行），例如：
        - 用张量并行拆分 Transformer 层的 QKV 计算，适配芯片的多核集群架构；
        - 用流水线并行处理超长序列，避免单芯片内存溢出；
    - **通信适配**：将框架的集合通信（如 AllReduce）绑定到芯片的互联协议（如 PCIe/NVLink 类似的自研链路），优化跨芯片数据传输。

### **三、算子层：高性能计算内核开发**

- **核心任务**：为 LLM 的关键算子（如注意力、矩阵乘法、激活函数）开发芯片专用实现，最大化硬件利用率。
- **设计要点**：
    - **算子映射**：将 Transformer 的核心计算（如 MatMul、Softmax）拆解为芯片支持的指令集（如张量计算单元 TCU 的专用指令）；
    - **算子优化**：
        - 利用芯片的存储层次（如片上 SRAM 缓存）减少访存延迟（如矩阵分块适配缓存大小）；
        - 算子融合（如 QKV 计算 + 注意力掩码融合），减少中间数据读写；
        - 稀疏计算优化（如跳过零值特征），适配芯片的稀疏加速单元；
    - **性能调优**：通过芯片性能计数器（如计算单元利用率、内存带宽）调整算子实现（如线程块大小、数据布局）。

### **四、编译层：模型到硬件指令的转换**

- **核心任务**：将 LLM 的计算图（由框架生成）编译为芯片可执行的机器码，完成优化（如指令重排、内存分配）。
- **设计要点**：
    - **计算图优化**：基于芯片架构进行图剪枝、算子合并（如将多层 BN+ReLU 合并为单指令）；
    - **指令生成**：通过编译器（如基于 TVM/MLIR 定制）将算子转换为芯片的微指令流，利用指令级并行（ILP）提升效率；
    - **内存调度**：优化数据在片上 / 片外存储的分配与搬运（如预取策略），避免计算单元空闲；
    - **硬件适配**：针对芯片的特殊功能（如动态电压调节、多精度计算）生成适配指令（如自动切换 FP16/INT8 计算模式）。

### **五、运行时（Runtime）层：任务调度与资源管理**

- **核心任务**：管理芯片的计算资源、内存和通信，协调多芯片 / 多节点的协同执行。
- **设计要点**：
    - **任务调度**：将编译后的指令分发到芯片的计算核心，支持多流并行（如计算与数据传输重叠）；
    - **内存管理**：
        - 分配芯片的 HBM/SRAM 资源（如为注意力权重分配高带宽存储）；
        - 实现内存池复用，减少动态分配开销；
    - **通信管理**：封装芯片间的互联接口（如自研高速链路），提供集合通信 API（如 AllReduce、Broadcast），支持分布式训练的梯度同步；
    - **故障处理**：检测芯片错误（如计算超时、内存错误），实现任务重试或故障节点隔离。

### **六、驱动层：硬件抽象与控制**

- **核心任务**：作为 Runtime 与硬件的接口，将高层指令转换为芯片的物理操作（如寄存器配置、时钟控制）。
- **设计要点**：
    - **硬件抽象**：封装芯片的底层寄存器、计算单元、存储控制器，提供统一的软件调用接口（如初始化、启动 / 停止计算）；
    - **资源隔离**：控制多进程 / 多任务对芯片资源的访问（如通过 PCIe BAR 空间隔离），避免冲突；
    - **性能监控**：读取芯片的传感器数据（如温度、功耗），反馈给 Runtime 进行动态调频（如高负载时提升核心频率）；
    - **兼容性**：适配 Linux 内核驱动框架（如 PCIe 设备驱动模型），确保芯片可被操作系统识别和管理。

### **七、硬件层：芯片物理实现与部署**

- **核心任务**：将上述软件层的需求映射到芯片的物理设计，最终部署为单机 / 集群系统。
- **设计要点**：
    - **计算单元**：根据 LLM 算子特性设计专用加速核（如 Transformer 计算引擎、注意力专用单元）；
    - **存储层次**：配置 HBM 容量（如 128GB / 芯片）和带宽（如 800GB/s），匹配 LLM 的访存需求；
    - **互联设计**：支持多芯片组网（如片间 NVLink-like 总线、RDMA 网络），满足分布式训练的通信带宽（如单机 8 卡总带宽 2TB/s）；
    - **功耗与散热**：根据软件层的计算强度（如峰值算力 3PFlops）设计电源和散热方案（如液冷），确保稳定运行。

### **总结：各层次的协同关系**

- **自顶向下**：算法层定义模型需求→框架层确定分布式策略→算子层适配计算核心→编译层优化指令与内存→Runtime 层调度资源→驱动层控制硬件→硬件层提供物理支撑。
- **自底向上**：硬件特性（如存储带宽）约束算子设计→编译层需匹配硬件指令集→Runtime 层需利用硬件互联特性→框架层的并行策略需适配硬件拓扑。

以 “千亿参数 LLM 推理” 为例，完整流程为：

1. 算法层确定用 INT4 量化压缩模型；
2. 框架层（vLLM）采用 PagedAttention 优化内存；
3. 算子层开发 INT4 注意力核，适配芯片的量化计算单元；
4. 编译层将算子合并为指令流，优化 HBM 访存；
5. Runtime 层调度多芯片分片执行，通过高速互联传输中间结果；
6. 驱动层控制芯片工作在低功耗模式，匹配推理场景；
7. 硬件层通过 8 卡集群提供足够算力，完成高吞吐推理。

每个层次的设计都需紧密围绕 “芯片特性” 与 “LLM 需求” 的匹配，最终实现模型在硬件上的高效部署。
