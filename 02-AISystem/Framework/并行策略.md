在大语言模型（LLM）的训练和推理中，常见的并行策略主要包括以下几种，它们通过不同的方式将计算任务分配到多设备上，以提升效率和扩展性。以下是每种策略的原理及适用部分的总结：

---

### **1. 张量并行（Tensor Parallelism, TP）**

#### **原理**：

- **核心思想**：将模型中的张量（如权重矩阵、激活值）分割到多个设备上，独立计算后再聚合结果。适用于单卡内存不足或计算量过大的场景。
- **分块方式**：
    - **列切分**：将矩阵按列划分，减少通信量（如MLP层的权重矩阵）。
    - **行切分**：将矩阵按行划分，需配合数据分片（如嵌入层）。
- **通信机制**：仅在需要完整张量时（如激活函数前或梯度聚合时）进行跨设备通信。
- **示例**：Megatron-LM 中的张量并行对MLP层和注意力层进行分块计算。

#### **适用部分**：

- **大矩阵运算**：如Transformer中的线性层（MLP、QKV投影）。
- **激活函数计算**：通过减少同步操作优化效率（如MLP层中的 `ACT(XA)B`）。
- **嵌入层**：按行切分嵌入矩阵，减少单卡显存占用。

---

### **2. 流水线并行（Pipeline Parallelism, PP）**

#### **原理**：

- **核心思想**：将模型按层垂直划分到多个设备（如将Transformer分片），设备间按顺序传递中间结果（类似流水线）。
- **微批处理（Micro-batch）**：将输入数据切分为小批次，设备在计算完前一层后立即传递结果到下一层，减少空闲时间（缓解“Pipeline Bubbles”）。
- **通信开销**：设备间需频繁传递激活值和梯度，通信成本较高。

#### **适用部分**：

- **模型分片**：适用于层数较多的模型（如深度Transformer堆叠）。
- **长序列处理**：结合微批处理优化长序列的前向/反向传播效率。

---

### **3. 序列并行（Sequence Parallelism）**

#### **原理**：

- **核心思想**：将输入序列按长度维度切分到多个设备，减少单卡显存占用。通过分布式计算激活值和注意力权重。
- **优化目标**：解决注意力机制的平方复杂度问题（如长序列的KV缓存存储）。
- **通信机制**：通过All-Gather或All-to-All操作聚合序列分片的中间结果。
- **示例**：Colossal-AI 的 Ring Self-Attention 和 DeepSpeed-Ulysses 的序列分片策略。

#### **适用部分**：

- **注意力计算**：减少KV缓存的显存占用（如长上下文生成）。
- **LayerNorm/Dropout**：序列维度分片降低冗余激活存储需求。

---

### **4. 数据并行（Data Parallelism, DP）**

#### **原理**：

- **核心思想**：将数据划分为小批次（mini-batch），每个设备独立计算梯度，最后通过平均梯度更新模型参数。
- **通信机制**：在反向传播后聚合各设备的梯度（AllReduce操作）。
- **局限性**：模型权重需复制到所有设备，显存占用高，不适用于超大规模模型。

#### **适用部分**：

- **训练阶段**：适合批量较大且模型较小的场景（如早期训练）。
- **结合ZeRO**：与ZeRO优化器结合（如ZeRO-3），减少显存占用（DeepSpeed）。

---

### **5. 混合并行（Hybrid Parallelism）**

#### **原理**：

- **组合策略**：结合TP、PP、序列并行等多种策略，最大化资源利用率。
- **示例**：Megatron-LM 的 TP+PP 组合，Colossal-AI 的 TP+序列并行+DP 组合。

#### **适用部分**：

- **超大规模模型**：如参数量超千亿的模型（TP分片模型，PP分片层，DP分片数据）。
- **长序列任务**：结合序列并行和TP优化KV缓存和激活存储。

---

### **6. 推理阶段的并行策略**

#### **1. Hogwild! Inference**

- **原理**：多个Worker共享KV缓存，通过并发注意力机制并行生成Token，无需严格顺序依赖。
- **适用部分**：自回归生成任务（如文本生成），通过弱依赖Token并行加速。

#### **2. 自回归生成的并行化**

- **原理**：将弱依赖的Token并行生成，保留强依赖的局部顺序生成（如PAR方法）。
- **适用部分**：图像/视频生成中的视觉Token并行。

#### **3. KV缓存优化**

- **原理**：通过压缩KV缓存（如SCOPE方法）减少长序列的存储和计算开销。
- **适用部分**：长上下文生成任务（如文档摘要、代码生成）。

---

### **总结对比**

|并行策略|核心目标|通信开销|适用场景|典型框架/论文|
|---|---|---|---|---|
|**张量并行**|减少单卡显存占用|中等|大矩阵运算（MLP、QKV）|Megatron-LM|
|**流水线并行**|扩展模型深度|高|深度Transformer分片|GPipe|
|**序列并行**|突破序列长度限制|高|长序列注意力计算|Colossal-AI, DeepSpeed|
|**数据并行**|加速训练|中等|小模型/大批量训练|PyTorch DataParallel|
|**混合并行**|兼顾模型规模和数据扩展|高|超大规模模型训练/推理|Megatron-LM + DeepSpeed|
|**Hogwild!**|推理加速|低|自回归生成任务|[6]|
|**KV缓存优化**|降低长序列显存占用|中等|长上下文生成|SCOPE [5]|

---

### **选择建议**

- **训练阶段**：优先采用 **TP+PP+DP** 的混合策略，结合ZeRO优化显存。
- **推理阶段**：针对长序列任务使用 **序列并行** 或 **KV缓存压缩**；对于生成任务，尝试 **Hogwild! Inference** 或 **自回归并行**。
- **超大规模模型**：必须采用 **混合并行**，并根据硬件资源动态调整TP/PP/DP比例。

这些策略可根据具体任务需求和硬件配置灵活组合，以实现最优的性能和资源利用率。


#  DeepEP

DeepSeek MoE 中使用的 DeepEP 通信加速库主要采用了以下并行策略，以优化 MoE 架构下的训练和推理效率：

---

### **1. 专家并行（Expert Parallelism, EP）**

- **核心思想**：将 MoE 中的专家网络（Experts）分布到不同的 GPU 或计算节点上，每个设备仅处理被激活的专家子集。
- **实现方式**：
    - **分发（Dispatch）**：将输入 Token 根据门控网络（Gate）的决策分发到对应专家所在的设备。
    - **聚合（Combine）**：将各设备上专家的输出结果加权合并回原设备。
- **优势**：
    - 扩展性强：支持大规模模型并行扩展（如 DeepSeek-V3 的 256 个专家）。
    - 计算效率高：每张卡仅处理部分专家，避免冗余计算。
- **适用场景**：MoE 模型的训练和推理阶段（尤其适合大规模专家数量场景）。

---

### **2. 高吞吐量通信策略**

- **节点内通信（Intra-node）**：
    - **NVLink 优化**：利用 NVLink 的高带宽（约 160GB/s）实现节点内 GPU 之间的全对全（All-to-All）通信。
    - **共享内存（IPC）**：通过 `ipc_handles` 和 `dist.all_gather_object` 快速同步数据。
- **节点间通信（Inter-node）**：
    - **RDMA 优化**：使用 RoCE/InfiniBand RDMA（最大带宽 ~50GB/s）实现跨节点的高吞吐量通信。
    - **异步调度**：通过 `nvshmem_int_put_nbi` 跨节点广播元数据，减少同步开销。
- **性能目标**：最大化数据传输吞吐量，适用于预填充（Prefill）等大规模数据处理场景。

---

### **3. 低延迟通信策略**

- **场景需求**：针对推理解码阶段的延迟敏感任务（如实时生成）。
- **实现方式**：
    - **RDMA IBGDA（InfiniBand Global Device Access）**：直接在 GPU 设备间通过 RDMA 进行低延迟通信。
    - **纯 RDMA 内核**：避免 CPU 中介，直接在 GPU 上完成数据传输。
- **优势**：
    - 显著降低通信延迟（如 DeepSeek-V3 生产设置中的解码任务）。
    - 支持 FP8/BF16 低精度数据传输，减少带宽占用。

---

### **4. 混合通信策略（Hybrid Communication）**

- **组合模式**：
    - **节点内**：优先使用 NVLink（高带宽）。
    - **跨节点**：切换为 RDMA（高吞吐 + 低延迟）。
- **动态适配**：根据任务类型（训练/推理、预填充/解码）自动选择通信模式。
- **代码实现**：
    - `Buffer.internode_dispatch` 和 `internode_combine` 方法实现跨节点通信。
    - `low_latency_dispatch` 和 `low_latency_combine` 方法用于低延迟场景。

---

### **5. 动态负载均衡**

- **问题背景**：MoE 中 Token 分配不均可能导致某些专家过载（过热），而其他专家空闲（遇冷）。
- **解决方案**：
    - **统计令牌分布**：在 `get_dispatch_layout` 内核中统计各专家和 Rank 的 Token 数量（`num_tokens_per_expert` 和 `num_tokens_per_rank`）。
    - **动态调整分区**：根据负载动态优化 Token 分发策略，避免专家过载。
- **效果**：提升整体计算资源利用率，减少冗余计算。

---

### **6. 通信与计算重叠**

- **关键机制**：
    - **异步通信与计算**：通过 CUDA 流（CUDA Stream）和事件（CUDA Event）实现通信与计算的并发执行。
    - **Hook-based 重叠**：在通信完成后立即触发专家计算，无需等待全局同步。
- **技术细节**：
    - 使用 `EventOverlap` 类管理通信与计算的重叠。
    - 支持 CUDA Graphs 编排任务，进一步优化执行顺序。
- **性能提升**：减少通信等待时间，提高 GPU 利用率。

---

### **7. 低精度操作支持**

- **数据类型**：支持 FP8 和 BF16 量化。
- **优势**：
    - 减少通信带宽需求（FP8 占用空间仅为 FP16 的一半）。
    - 降低显存占用，提升计算效率。
- **应用场景**：训练和推理中的低精度计算（如 DeepSeek-V3 的 FP8 量化策略）。

---

### **总结：DeepEP 的核心策略**

|并行策略|核心目标|通信技术|适用场景|
|---|---|---|---|
|**专家并行（EP）**|分布式处理专家网络|All-to-All|MoE 训练/推理|
|**高吞吐量通信**|最大化数据传输速率|NVLink/RDMA|预填充阶段|
|**低延迟通信**|降低推理解码延迟|RDMA IBGDA|解码阶段|
|**混合通信**|适配不同带宽场景|NVLink + RDMA|训练/推理全流程|
|**动态负载均衡**|均衡专家负载|Token 统计优化|防止专家过热/遇冷|
|**通信-计算重叠**|减少空闲时间|CUDA 流/事件|全流程|
|**低精度操作**|降低带宽和显存占用|FP8/BF16|训练/推理优化|

---

### **实际效果**

- **训练成本降低**：DeepSeek-V3 的训练成本约为 557.6 万美元，远低于 GPT-4 的 7800 万美元。
- **吞吐量提升**：通过双批次重叠（Dual-Batch Overlap）策略，预填充阶段吞吐量显著提高。
- **延迟优化**：低延迟通信模式使解码阶段响应速度提升，支持实时生成需求。

DeepEP 通过上述策略的组合，为 MoE 模型提供了高效、灵活的通信解决方案，尤其适合大规模分布式训练和低延迟推理场景。

# MoE 并行

DeepSeek MoE 层在其架构设计中采用了多种并行策略，以优化大规模混合专家（MoE）模型的训练和推理效率。以下是其核心并行策略及其原理：

---

### **1. 专家并行（Expert Parallelism, EP）**

- **核心思想**：将MoE中的多个专家网络（Experts）分布到不同的GPU或计算节点上，每个设备仅处理被激活的专家子集。
- **实现方式**：
    - **分发（Dispatch）**：根据门控网络的决策，将输入Token分配到对应的专家所在的设备。
    - **聚合（Combine）**：将各设备上专家的输出结果加权合并回原设备。
- **优势**：
    - **扩展性强**：支持大规模专家数量（如DeepSeek-V3的256个专家）。
    - **计算效率高**：每张卡仅处理部分专家，避免冗余计算。
- **冗余专家部署**：通过动态复制高负载专家，缓解专家过载问题（如DeepEP中的冗余专家策略）。

---

### **2. 动态负载均衡**

- **分层负载均衡（Hierarchical Load Balancing）**：
    - **适用场景**：预填充阶段（Prefilling Stage）。
    - **策略**：将专家组均匀分配到各节点，再在节点内复制专家副本，确保节点间和GPU间的负载均衡。
- **全局负载均衡（Global Load Balancing）**：
    - **适用场景**：解码阶段（Decoding Stage）。
    - **策略**：全局复制专家（忽略专家组限制），将副本分配到各GPU，优化大规模并行时的负载均衡。
- **实现**：通过开源的 `eplb.py` 库（如 `eplb.rebalance_experts` 函数）动态调整专家分布。

---

### **3. 高吞吐量与低延迟通信**

- **节点内通信**：
    - **NVLink优化**：利用NVLink的高带宽（约160GB/s）实现全对全（All-to-All）通信。
    - **共享内存（IPC）**：通过 `ipc_handles` 和 `dist.all_gather_object` 快速同步数据。
- **节点间通信**：
    - **RDMA优化**：使用RoCE/InfiniBand RDMA（最大带宽 ~50GB/s）实现跨节点高吞吐量通信。
    - **低延迟模式**：通过RDMA IBGDA（InfiniBand Global Device Access）直接在GPU间进行低延迟通信，适用于推理解码场景。

---

### **4. 通信与计算重叠**

- **异步调度**：
    - **CUDA流与事件**：通过CUDA流（Stream）和事件（Event）实现通信与计算的并发执行。
    - **Hook-based重叠**：在通信完成后立即触发专家计算，减少同步等待时间。
- **Dual-Batch Overlap**：通过双批次重叠策略（如DeepEP中的 `Buffer.internode_dispatch` 和 `internode_combine`）进一步提升吞吐量。

---

### **5. 低精度操作支持**

- **FP8量化**：
    - **激活值缓存**：在FP8中存储激活值，减少内存占用。
    - **Wgrad GEMM优化**：允许FP8格式的矩阵乘法（GEMM），提升计算速度。
- **BF16混合精度**：对关键操作（如嵌入模块和注意力层）保持高精度，确保训练稳定性。

---

### **6. 冗余专家与专家共享**

- **冗余专家部署**：
    - **动态复制**：根据在线统计的专家负载，定期复制高负载专家到其他GPU，减少all-to-all通信开销。
    - **资源动态分配**：在文本生成任务中，动态调整专家资源分配，避免全模型计算。
- **专家共享机制**：
    - **共享参数**：部分专家在不同层或Token间共享参数，减少专家数量（如DeepSeek-V3中8个共享专家）。

---

### **7. 混合并行策略**

- **EP + 数据并行（DP）**：
    - **门控网络复制**：将门控网络复制到所有设备，确保路由决策一致性。
    - **专家网络分片**：专家网络分布到不同设备，结合数据并行提高训练效率。
- **EP + 流水线并行（PP）**：
    - **分层流水线**：将Transformer层与MoE层结合，通过流水线并行（如16路PP）提升计算效率。

---

### **总结：DeepSeek MoE 层的并行策略**

|并行策略|核心目标|通信技术|适用场景|
|---|---|---|---|
|**专家并行（EP）**|分布式处理专家网络|All-to-All|MoE 训练/推理|
|**动态负载均衡**|均衡专家负载|Token 统计优化|防止专家过热/遇冷|
|**高吞吐量通信**|最大化数据传输速率|NVLink/RDMA|预填充阶段|
|**低延迟通信**|降低推理解码延迟|RDMA IBGDA|解码阶段|
|**通信-计算重叠**|减少空闲时间|CUDA 流/事件|全流程|
|**低精度操作**|降低带宽和显存占用|FP8/BF16|训练/推理优化|
|**冗余专家与共享**|提高资源利用率|动态复制/共享参数|文本生成/大规模MoE|
|**混合并行**|适配不同任务需求|EP+DP/PP|训练/推理全流程|

---

### **实际效果**

- **训练效率提升**：通过冗余专家和负载均衡策略，DeepSeek-V3的训练成本仅为557.6万美元（对比GPT-4的7800万美元）。
- **推理性能优化**：MLA KV缓存压缩和FP8量化显著降低内存占用，提升推理速度。
- **扩展性增强**：支持超大规模专家数量（如256个专家），并兼容长序列生成任务。

这些策略的组合使DeepSeek MoE在保持高性能的同时，显著降低了计算和通信开销，成为大规模MoE模型的高效解决方案。