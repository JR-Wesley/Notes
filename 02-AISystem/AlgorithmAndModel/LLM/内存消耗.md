---
dateCreated: 2025-08-17
dateModified: 2025-08-17
---
在计算大语言模型（LLM）的资源占用时，通常需要考虑以下几个关键组成部分，它们分别代表模型运行和训练过程中的不同数据：

1. **Param（模型参数，Parameters）**

    - 指模型本身的核心参数，包括权重（weights）和偏置（biases），是模型通过训练学习到的核心数据。
    - 例如，Transformer 中的注意力矩阵、线性层权重等都属于 param。
    - 参数量是 LLM 最核心的指标（如 “100 亿参数模型”），直接决定了模型的容量和表达能力。
2. **Activation（激活值）**

    - 指模型在**推理或训练过程中**，输入数据经过各层计算后产生的中间输出值。
    - 例如，输入文本经过嵌入层（Embedding）、注意力层、前馈网络等计算后生成的张量，都属于激活值。
    - 激活值是临时的，随输入数据变化而变化，推理结束后通常会被释放，但在模型运行时会占用内存。
3. **Gradient（梯度）**

    - 指训练过程中，损失函数对模型参数的偏导数，用于通过反向传播更新参数。
    - 每个参数对应一个梯度值，其维度与参数相同。
    - 仅在训练阶段存在，推理阶段不需要计算梯度（可通过 “推理模式” 关闭）。
4. **Optimizer State（优化器状态）**

    - 指优化器（如 Adam、SGD）在训练过程中维护的额外数据，用于调整参数更新的策略。
    - 例如，Adam 优化器会保存每个参数的一阶矩（动量）和二阶矩（自适应学习率相关），这些都属于优化器状态。
    - 优化器状态的大小通常与参数量相当（如 Adam 的状态大小约为参数的 2 倍），仅在训练阶段存在。

**总结**：

- 推理阶段（仅做预测）：主要占用资源的是 **param** 和 **activation**。
- 训练阶段：还需要额外存储 **gradient** 和 **optimizer state**，因此训练时的内存需求通常是推理时的 3-4 倍（取决于优化器）。

这些概念在计算模型的内存占用、选择硬件配置（如 GPU 显存）时非常重要。
