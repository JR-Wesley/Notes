---
dateCreated: 2025-08-09
dateModified: 2025-08-09
---

_**Megatron-Coreï¼šPP åŸºæœ¬åŽŸç†ï¼ˆæœ´ç´ æµæ°´å¹¶è¡ŒåŽŸç†ï¼ŒBubble ç©ºæ³¡çŽ‡è®¡ç®—ï¼ŒGpipe åŽŸç†è§£æžä¸ŽåŠ¨æ€å†…å­˜å³°å€¼åˆ†æžï¼‰**_

**1ï¼‰æœ´ç´ æµæ°´å¹¶è¡ŒåŽŸç†ï¼š** æœ´ç´ æµæ°´å¹¶è¡ŒåŽŸç†è¾ƒä¸ºç®€å•ï¼Œå¦‚å›¾æ‰€ç¤ºï¼šå››ç§ä¸åŒçš„è‰²å—ä»£è¡¨ä¸åŒçš„ rankï¼ˆæˆ– GPUï¼‰ï¼Œrank0 å°†æœ¬åœ°è®¡ç®—åŽçš„æ¿€æ´»å€¼ä¼ é€’ç»™åŽç»­çš„ rank1ï¼Œä»¥æ­¤ç±»æŽ¨ç›´åˆ° rank3 å®Œæˆç±»ä¼¼æµæ°´çº¿çš„å‰å‘ä¼ è¾“è¿‡ç¨‹ï¼Œæ­¤æ—¶ rank3 å¼€å¯åå‘æµæ°´çº¿è®¡ç®—è¿‡ç¨‹ï¼Œå°†æœ¬é˜¶æ®µçš„è®¡ç®—æ¢¯åº¦ä¼ é€’ç»™ rank2ï¼Œä»¥æ­¤ç±»æŽ¨å®Œæˆåå‘ä¼ è¾“è¿‡ç¨‹ï¼Œæœ€ç»ˆå¾—åˆ°å„ä¸ª rank çš„æ¢¯åº¦ï¼Œè¿›è¡Œæ¨¡åž‹å„ä¸ªæµæ°´çº¿é˜¶æ®µçš„æƒé‡å‚æ•°æ›´æ–°ï¼Œæ­¤æ—¶å®Œæˆä¸€è½®æµæ°´çº¿è¿­ä»£è®­ç»ƒã€‚[![alt text](https://github.com/Infrasys-AI/AIInfra/raw/main/04Train/02ParallelAdv/image.png)](https://github.com/Infrasys-AI/AIInfra/blob/main/04Train/02ParallelAdv/image.png)

**2ï¼‰Bubble ç©ºæ³¡çŽ‡è®¡ç®—ï¼š** æ­¤æ—¶æ˜¾ç„¶å¯è§ï¼Œå›¾ä¸­å¤§éƒ¨åˆ†çš„æ—¶é—´ä¸ºç©ºç™½ï¼Œè®¡ç®—ä¸Žé€šä¿¡ç¼ºä¹ overlapï¼Œå³å­˜åœ¨è®¡ç®—ç­‰é€šä¿¡çš„çŽ°è±¡ï¼ˆrank1 è¦ç­‰å¾… rank0 æ¿€æ´»å‰å‘ä¼ é€’ä¹‹åŽæ‰èƒ½è®¡ç®—ï¼‰ï¼Œå› æ­¤å¯¹äºŽå›¾ä¸­çš„â€œç©ºç™½éƒ¨åˆ†â€ï¼Œæˆ‘ä»¬å¼•å…¥ Bubble çš„æ¦‚å¿µæ¥å®šé‡çš„è¯„ä¼°æµæ°´çº¿å¹¶è¡Œçš„æ€§èƒ½ã€‚ç©ºæ³¡ Bubble çš„äº§ç”Ÿï¼Œæ˜¯å› ä¸ºç®—å­å†…å¹¶è¡Œå’Œç®—å­é—´å¹¶è¡Œè§£å†³å•è®¾å¤‡å†…å­˜ä¸è¶³çš„é—®é¢˜ï¼Œæ¨¡åž‹ç»è¿‡æ‹†åˆ†åŽçš„ä¸Šä¸€ä¸ª rank çš„ stage éœ€è¦é•¿æœŸæŒç»­å¤„äºŽç©ºé—²çŠ¶æ€ï¼Œç­‰å¾…å…¶ä»– rank çš„ stage è®¡ç®—å®Œæˆï¼Œæ‰å¯ä»¥å¼€å§‹è®¡ç®—ï¼Œè¿™æžå¤§é™ä½Žäº†è®¾å¤‡çš„å¹³å‡ä½¿ç”¨çŽ‡ã€‚è¿™ç§çŽ°è±¡è¢«ç§°ä¸ºå¹¶è¡Œç©ºæ³¡ï¼ˆParallelism Bubbleï¼‰ã€‚æ€»çš„ bubble å ç”¨çš„æ—¶é—´è·Ÿæµæ°´å¹¶è¡Œ PP åˆ‡åˆ†ç­–ç•¥ç›¸å…³ï¼š

$$
\begin{equation} t_{bubble} = (p - 1)(t_f + t_b) \end{equation}
$$ å…¶ä¸­ p ä¸ºå¹¶è¡Œåº¦ï¼Œ$t_f$ ä¸ºå‰å‘æ—¶é—´ï¼Œ$t_b$ ä¸ºåå‘æ—¶é—´ï¼Œpipeline bubble å æ®äº† ( p âˆ’ 1 ) ä¸ªå‰å‘ã€åå‘è¿‡ç¨‹ã€‚ Bubble å æœ‰çŽ‡æ¯”ä¾‹ bubbletaionï¼Œåˆç§°ç©ºæ³¡çŽ‡ï¼Œè®¡ç®—ç”±å…¬å¼ç»™å‡ºï¼š
$$

\begin{equation} \mathit{bubble ration} = \frac{t_{bubble}}{t_{bubble} + t_{ideal}} = \frac{(p-1)(t_f + t_b)}{(p-1)(t_f + t_b) + m(t_f + t_b)} = \frac{p - 1}{m + p - 1} \end{equation}

$$ å…¶ä¸­ t i d e a l ä¸ºç†æƒ³è¿­ä»£æ—¶é—´ï¼Œ$m$ ä¸º micro-batch çš„æ•°é‡ï¼Œ$t_f$ï¼Œ$t_b$ ä¸ºå•ä¸ª m i c r o âˆ’ b a t c h æ—¶é—´ï¼Œå› æ­¤ t i d e a l = m ( t f + t b )ã€‚æ ¹æ®ä¸Šé¢çš„å…¬å¼ï¼Œ$bubble ration$ è·Ÿ m i c r o âˆ’ b a t c h e s æœ‰å…³ç³»ï¼Œmicro-batch æ•°é‡ï¼ˆmï¼‰è¶Šå¤šï¼ŒBubble çš„æ¯”ä¾‹è¶Šä¼šé™ä½Žåˆ°å¯æŽ¥å—çš„æ°´å¹³ï¼Œå› æ­¤åœ¨è¡¡é‡å¤§æ¨¡åž‹æ€§èƒ½ä¼˜åŒ–çš„è¿‡ç¨‹ï¼Œ$bubble ration$ æ˜¯ä½œä¸ºä¸€ä¸ªè¡¡é‡æŒ‡æ ‡åŽ»çœ‹å¾…å…¶åˆ©ç”¨çŽ‡ã€‚

**3ï¼‰Gpipe åŽŸç†è§£æžï¼š** Gpipe æ˜¯åŸºäºŽä¸Šè¿°ç‰¹ç‚¹æŽ¨å‡ºçš„æµæ°´çº¿å¹¶è¡ŒæŠ€æœ¯ï¼šå°†ä¸€ä¸ª batch size çš„æ•°æ®åˆ‡åˆ†æˆå››ä¸ª micro-batch sizeï¼Œæ¯ä¸ª micro-batch ä½œä¸ºæœ´ç´ æµæ°´å¹¶è¡Œæ–¹å¼ä¸­çš„ä¸€ä¸ª batchï¼Œå‰å‘è¿‡ç¨‹ä»Ž rank0 æµå‘ rank3ï¼ˆåˆç§°ä¸º warmupï¼‰ï¼Œå†åå‘å›žæº¯ï¼ˆç§°ä¸º cooldownï¼‰ã€‚[![alt text](https://github.com/Infrasys-AI/AIInfra/raw/main/04Train/02ParallelAdv/Gpipe.png)](https://github.com/Infrasys-AI/AIInfra/blob/main/04Train/02ParallelAdv/Gpipe.png) è®¡ç®—ç©ºæ³¡çŽ‡ï¼š

$$
\begin{equation} bubble ration=\frac{t_{bubble}}{t_{ideal}}=\frac{p-1}{m} \end{equation}
$$ å› æ­¤ä¸ºé™ä½Žç©ºæ³¡çŽ‡ï¼Œé€šå¸¸éœ€è¦å¢žåŠ æ•°æ®åˆ‡åˆ† micro-batches çš„æ•°é‡ mï¼Œå³ä»¤ m >> p . åœ¨æ¨¡åž‹çš„åå‘ä¼ è¾“è¿‡ç¨‹ä¸­ï¼Œç”±äºŽ GPU éœ€è¦ä¿å­˜å‰å‘ä¼ æ’­æ—¶çš„ä¸­é—´æ¿€æ´»å€¼ï¼Œä»¥ä¾¿è®¡ç®—æ¢¯åº¦ï¼Œå› æ­¤åˆ’åˆ† micro-batch çš„æ•°ç›® m å°†ç”±å• GPU è®¡ç®—å¡çš„æ˜¾å­˜çº¦æŸï¼ˆeg. ç›¸åŒè‰²å—ä¸ºä¸€å¼  GPU,å¯¹äºŽ GPU1 æ¥è¯´éœ€è¦ä¿å­˜ m=4 ä¸ªå‰å‘è¿‡ç¨‹çš„æ¿€æ´»å€¼ï¼Œå› æ­¤å½“ä½¿ç”¨å¤šä¸ª micro-batchï¼Œæ¿€æ´»å€¼å­˜å‚¨é‡çº¿æ€§å¢žåŠ ï¼‰ï¼Œåœ¨ warmup é˜¶æ®µç»“æŸåŽæ‰€æœ‰ GPU æ˜¾å­˜ï¼Œè¾¾åˆ°ç§°ä¸ºåŠ¨æ€å†…å­˜å³°å€¼ã€‚

**4ï¼‰åŠ¨æ€å†…å­˜å³°å€¼åˆ†æžï¼š** ä¸ºè§£å†³ Gpipe å¸¦æ¥çš„åŠ¨æ€å†…å­˜å³°å€¼é—®é¢˜ï¼Œé‡è®¡ç®—ï¼ˆRecomputationï¼‰æŠ€æœ¯è¢«å¼•å…¥è§£å†³æ˜¾å­˜ç“¶é¢ˆé—®é¢˜ï¼Œå…¶æ ¸å¿ƒæ€æƒ³ä¸ºï¼šä¸Žå…¶åœ¨å‰å‘ä¼ æ’­ä¸­ç¼“å­˜æ‰€æœ‰ä¸­é—´æ¿€æ´»å€¼ï¼Œä¸å¦‚åœ¨åå‘ä¼ è¾“æ—¶â€œé‡æ–°è®¡ç®—â€ä¸€éå‰å‘è¿‡ç¨‹ï¼Œæ¥èŽ·å¾—éœ€è¦çš„æ¿€æ´»å€¼ï¼Œä»Žè€ŒèŠ‚çœæ˜¾å­˜ã€‚ (å¾…è¡¥å……)



# æµæ°´å¹¶è¡Œ 1F1B/1F1B Interleaved åŽŸç†

abstractï¼šå…ˆå‰ä»‹ç»çš„ Gpipe å­˜åœ¨ç¡¬ä»¶åˆ©ç”¨çŽ‡ä½Žï¼ŒåŠ¨æ€å†…å­˜åŽ‹åŠ›å¤§çš„é—®é¢˜ï¼Œæœ¬ç¯‡ä»‹ç»æ–°çš„æµæ°´çº¿æŠ€æœ¯æ¥è§„é¿

## PipeDream åŸºæœ¬åŽŸç†

å›žé¡¾ä¸€ä¸‹ Gpipe æµæ°´å¹¶è¡Œå­˜åœ¨åŠ¨æ€å³°å€¼å†…å­˜å¤§çš„é—®é¢˜ï¼Œå¦‚å›¾æ‰€ç¤ºï¼šè‹¥è¾“å…¥ batch è¢«åˆ’åˆ†ä¸º n ä¸ª micro-batchï¼Œåˆ™å¯¹äºŽä»»æ„ deviceï¼Œéœ€è¦ç¼“å­˜ n ä»½å‰å‘æ¿€æ´»å€¼ï¼ˆå›¾ä¸­ n=8ï¼‰.

![](assets/10_pipeline.assets/10pipeline01.png)

PipeDream æµæ°´çº¿å¹¶è¡Œé‡‡å–äº†**1FIB**çš„ç­–ç•¥ï¼Œå¾ˆå¥½çš„è§„é¿äº†ç¡¬ä»¶å†…å­˜æœ‰é™çš„é—®é¢˜ã€‚

åœ¨æµæ°´çº¿å¹¶è¡Œï¼ˆpipeline parallelï¼‰ä¸­ï¼Œæ¯æ¬¡å‰å‘è®¡ç®—äº§ç”Ÿçš„ activation åªæœ‰åœ¨å¯¹åº”çš„åå‘è®¡ç®—å®Œæˆä¹‹åŽæ‰èƒ½é‡Šæ”¾ï¼ˆå³ä½¿ä½¿ç”¨äº† Checkpointing æŠ€æœ¯ï¼‰ã€‚å› æ­¤ï¼Œè¦å°½å¯èƒ½åœ°èŠ‚çœ activation å ç”¨çš„æ˜¾å­˜ï¼Œå°±éœ€è¦å°½é‡ç¼©çŸ­æ¯ä»½ activation åœ¨å†…å­˜ä¸­åœç•™çš„æ—¶é—´ï¼Œä¹Ÿå°±æ˜¯è®©å®ƒä»¬å°½æ—©è¢«é‡Šæ”¾ã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œå…³é”®ä¾¿æ˜¯è®©æ¯ micro-batch çš„åå‘è®¡ç®—å°½æ—©å¼€å§‹å¹¶å®Œæˆã€‚å…·ä½“åšæ³•æ˜¯ï¼Œå°†åå‘è®¡ç®—çš„ä¼˜å…ˆçº§è°ƒé«˜ï¼Œä½¿å¾—ç¼–å·è¾ƒå°çš„ micro-batch çš„åå‘æ­¥éª¤ï¼Œèƒ½åœ¨ç¼–å·è¾ƒå¤§çš„ micro-batch çš„å‰å‘æ­¥éª¤ä¹‹å‰æ‰§è¡Œã€‚ä»¥ä¸€ä¸ªå¤šé˜¶æ®µï¼ˆstageï¼‰æµæ°´çº¿ä¸ºä¾‹ï¼šå¦‚æžœæˆ‘ä»¬è®©æœ€åŽä¸€ä¸ª stage åœ¨å®Œæˆå½“å‰ micro-batch çš„å‰å‘è®¡ç®—åŽï¼Œç«‹åˆ»å¯åŠ¨è¯¥ micro-batch çš„åå‘è®¡ç®—ï¼Œé‚£ä¹ˆåŽç»­çš„å„ä¸ª stage å°±èƒ½æ›´æ—©åœ°æ”¶åˆ°åå‘è®¡ç®—çš„æ•°æ®ï¼Œè¿›è€Œå¼€å§‹å®ƒä»¬è‡ªå·±çš„åå‘è®¡ç®—ã€‚é€šè¿‡è¿™ç§â€œå‰å‘åšä¸€æ‰¹ã€åå‘ç´§è·Ÿä¸€æ‰¹â€ï¼ˆ1F1B one-forward-one-backwardï¼‰çš„è°ƒåº¦ç­–ç•¥ï¼Œä¸ä»…èƒ½å¤Ÿå‡å°‘ activation åœ¨æ˜¾å­˜ä¸­çš„æ»žç•™æ—¶é—´ï¼Œè¿˜èƒ½å¹³è¡¡å„ä¸ª stage çš„è®¡ç®—è´Ÿè½½ï¼Œæœ€ç»ˆæœ€å¤§åŒ–æ˜¾å­˜åˆ©ç”¨æ•ˆçŽ‡å¹¶é™ä½Žæ•´ä½“è®­ç»ƒæ—¶çš„å†…å­˜å³°å€¼éœ€æ±‚ã€‚

å› æ­¤æˆ‘ä»¬å®žçŽ°äº†å°†æ¿€æ´»å€¼æ•°é‡ä¸Šé™ä»Ž micro-batch æ•°é‡ **m** å˜æˆ pipeline stage é˜¶æ®µ **p**ï¼Œä½†åªæ˜¯é™ä½Žäº†è®¾å¤‡çš„å³°å€¼å†…å­˜ï¼Œå¹¶æ²¡æœ‰é™ä½Žæ°”æ³¡å¤§å°ï¼Œå› æ­¤ç©ºæ³¡çŽ‡ä¸Ž Gpipe ä¿æŒä¸€è‡´ï¼Œä¸ºï¼š

$$

 \begin{equation} bubble ration=\frac{t_{bubble}}{t_{ideal}}=\frac{p-1}{m} \end{equation}

$$

![](assets/10_pipeline.assets/10pipeline02.png)

## Virtual Pipeline åŸºæœ¬åŽŸç†

åŽç»­ Megatron-LM åœ¨ 1F1B çš„åŸºç¡€ä¸Šåšäº† Interleaved 1F1B çš„ä¼˜åŒ–ï¼Œå‡å°‘äº†æµæ°´çº¿æ°”æ³¡ï¼Œä¹Ÿå°±æ˜¯æœ¬ç¯‡ä»‹ç»çš„è™šæ‹Ÿæµæ°´å¹¶è¡Œï¼ˆVirtual Pipeline Parallelismï¼Œç®€ç§° VPPï¼‰ã€‚

VPP çš„æ ¸å¿ƒåœ¨äºŽï¼Œè®©ä¸€ä¸ªç‰©ç†å±‚é¢çš„ device è™šæ‹Ÿæˆä¸º v ä¸ª devicesï¼Œdevice ä»Žè®¡ç®— 1 ä¸ªæˆ–è¿žç»­ layer æ®µåˆ°è®¡ç®— v ä¸ªä¸ç›¸é‚»çš„ layerï¼Œå¦‚å›¾æ‰€ç¤ºï¼šGPU1 ä¹‹å‰åªè´Ÿè´£ layer1 æˆ– layer1+layer2 å±‚çš„è®¡ç®—ï¼Œç»è¿‡è™šæ‹ŸåŒ–æµæ°´çº¿åŽï¼Œè´Ÿè´£ layer0 å’Œ layer5 å±‚çš„è®¡ç®—ï¼Œä½¿å¾— layer1 å±‚è®¡ç®—å®ŒæˆåŽæ— éœ€ç­‰å¾… layer2 çš„è®¡ç®—ï¼Œå¯ä»¥ç›´æŽ¥è¿›å…¥ GPU2 è¿›è¡Œè®¡ç®—ï¼Œä»Žè€Œå‡å°‘ç­‰å¾…ç©ºæ³¡æ—¶é—´ï¼Œæ­¤å¤„ v è¢«ç§°ä¸ºè™šæ‹Ÿæµæ°´çº¿é˜¶æ®µï¼ˆvirtual pipeline stageï¼‰ã€‚

![](assets/10_pipeline.assets/10pipeline03.png)

å‡è®¾æ¨¡åž‹æ€»å±‚æ•°ä¸º 16ï¼Œå¼ é‡å¹¶è¡Œå¤§å° tp=1ï¼Œæµæ°´çº¿å¹¶è¡Œå¤§å° pp=4ï¼Œè™šæ‹Ÿæµæ°´çº¿å¹¶è¡Œå¤§å° v=2ï¼Œåˆ™æ¨¡åž‹å°†è¢«åˆ’åˆ†ä¸º 4 * 2 = 8 ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µåŒ…å« 16 / 8 = 2 ä¸ªå±‚ã€‚å‰å‘çš„é¡ºåºä¸º GPU 1 -> GPU 2 -> GPU 3 -> GPU 4 -> GPU 1 -> GPU 2 -> GPU 3 -> GPU 4ã€‚åœ¨è®¾å¤‡æ•°é‡ä¸å˜çš„æƒ…å†µä¸‹ï¼Œåˆ†å‡ºæ›´å¤šçš„æµæ°´çº¿é˜¶æ®µï¼Œè¿™æ ·å¯ä»¥è®©æµæ°´çº¿ä¸­æ¯ä¸ª stage æ›´å°ï¼Œå› è€Œä¸‹ä¸ª stage çš„ç­‰å¾…æ—¶é—´æ›´çŸ­ï¼Œæ°”æ³¡æ›´å°ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œm éœ€è¦æ˜¯ p çš„æ•´æ•°å€ã€‚

![](assets/10_pipeline.assets/10pipeline04.png)

ð‘šä¸º micro-batchï¼Œð‘ä¸º pipeline stagesï¼Œv ä¸º virtual pipeline stage,å®Œæˆ v ä¸ª layer æ®µä¸­ä¸€ä¸ªçš„å‰å‘ã€åŽå‘æ—¶é—´åˆ†åˆ«ä¸º $t_f/v$ å’Œ $t_b/v$,æµæ°´çº¿æ°”æ³¡çš„è€—æ—¶ $t_{pd}^{int}$:

$$

 \begin{equation} t_{pd}^{int}=\frac{(p-1)_(t_f+t_b)}{v} \end{equation}

$$
 å› æ­¤å¯å¾—å‡ºVPPçš„ç©ºæ³¡çŽ‡ï¼š
$$

 \begin{equation} bubble ration=\frac{1}{v}_\frac{p-1}{m} \end{equation}

$$ ç©ºæ³¡çŽ‡é™¤äº†è·Ÿ micro batch æˆåæ¯”ï¼Œä¸Ž v ä¹Ÿæˆåæ¯”ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒVPPæ˜¯ä»¥å¢žåŠ é€šä¿¡é‡ä¸ºä»£ä»·ï¼Œæ¢å–æ›´ä½Žçš„ç©ºæ³¡æ¯”çŽ‡ï¼Œç›¸æ¯”äºŽ1FBçŽ°åœ¨çš„æ°”æ³¡å æ¯”å°±å‡å°‘åˆ°äº†1/vã€‚ä½†æ˜¯æµæ°´çº¿ä¹‹é—´çš„é€šä¿¡é‡ä¹Ÿå¢žåŠ äº†vå€ã€‚å¯¹äºŽä¸€ä¸ª pipeline stageï¼Œé‡Œé¢åŒ…æ‹¬å¤šä¸ª Transformer layerï¼Œæ‰€ä»¥çŽ°åœ¨ç›¸å½“äºŽæµæ°´çº¿çš„stageå¢žåŠ äº†ï¼Œé€šä¿¡é‡ä¹Ÿä¼šå¢žåŠ ã€‚ç‰¹åˆ«æ˜¯å½“globalçš„batchè¶Šæ¥è¶Šå¤§çš„æ—¶å€™ï¼Œè¿™ä¸ªé€šä¿¡å¼€é”€å°±ä¼šæ›´æ˜¾è‘—ã€‚


## æ–°å…´PPæŠ€æœ¯ï¼ˆæ‰©å……ï¼‰
eam-2BW


PipeDream-2BW æ˜¯ä¸€ç§é¢å‘è¶…å¤§è§„æ¨¡æ·±åº¦æ¨¡åž‹çš„å¼‚æ­¥æµæ°´çº¿å¹¶è¡Œæ–¹æ³•ï¼šå®ƒå°†æ¨¡åž‹åˆ‡åˆ†ä¸ºå¤šä¸ªé˜¶æ®µå¹¶å¤åˆ¶å¤šè·¯æµæ°´çº¿ï¼Œåœ¨ 1F1B è°ƒåº¦ä¸‹é€šè¿‡â€œåŒç¼“å†²â€æƒé‡æ›´æ–°å’Œæ¢¯åº¦åˆå¹¶æŠ€æœ¯ï¼Œå¤§å¹…é™ä½Žæ˜¾å­˜å ç”¨ä¸Žé€šä¿¡å¼€é”€ï¼›å†…ç½®çš„è‡ªåŠ¨åŒ– Planner æ ¹æ®è®¾å¤‡å†…å­˜å’Œäº’è”æ‹“æ‰‘æœç´¢æœ€ä¼˜é˜¶æ®µåˆ’åˆ†ä¸Žå¤åˆ¶å®½åº¦ï¼Œå¹¶å¯é€‰æ¿€æ´»é‡è®¡ç®—ï¼›åœ¨å¤šå¡é›†ç¾¤ä¸Šè®­ç»ƒå¤§è§„æ¨¡ Transformer æ¨¡åž‹æ—¶ï¼Œç›¸è¾ƒäºŽä¼ ç»Ÿæµæ°´çº¿å¹¶è¡Œï¼Œåžåé‡å¯æå‡æ•°å€ï¼ŒåŒæ—¶ä¿ç•™ä¸Žæ•°æ®å¹¶è¡Œä¸€è‡´çš„æƒé‡æ›´æ–°è¯­ä¹‰ã€‚ 

![](assets/10_pipeline.assets/10pipeline05.png)
### ZB-V Schedule


ZB-V schedule æ˜¯ä¸€ç§é¢å‘æµæ°´çº¿å¹¶è¡Œçš„å†…å­˜é«˜æ•ˆé›¶æ°”æ³¡è°ƒåº¦ç­–ç•¥ï¼šå®ƒå°†pä¸ªé˜¶æ®µåˆ’åˆ†ä¸º2pä¸ªæ¨¡åž‹å—ï¼Œå¹¶ç»™æ¯ä¸ª worker åˆ†é…ä¸¤ä¸ªæ¨¡åž‹å—ï¼ŒæŒ‰ç…§ä»Žé¦–åˆ°å°¾å†è¿”å›žé¦–çš„Våž‹é¡ºåºè¿›è¡Œåˆ†é…ï¼Œä»¥ç¡®ä¿æ¯ä¸ªå¾®æ‰¹æ¬¡çš„å‰å‘å’Œå¯¹æƒé‡çš„åŽå‘éƒ½åœ¨åŒä¸€workerä¸Šæ‰§è¡Œï¼Œä»Žè€Œåˆ©ç”¨åŽå‘æƒé‡è®¡ç®—å¡«å……æµæ°´çº¿ç©ºéš™ï¼›åœ¨ä¸Ž1F1Bç›¸åŒçš„æ˜¾å­˜çº¦æŸä¸‹ï¼Œå¯åœ¨æ­£å‘ã€åŽå‘è¾“å…¥ä¸Žæƒé‡åŽå‘è®¡ç®—æ—¶é—´ç›¸ç­‰æ—¶å®žçŽ°è¿‘é›¶æ°”æ³¡ï¼›åŒæ—¶ï¼Œè¯¥è°ƒåº¦ä¿æŒå„ worker å³°å€¼æ¿€æ´»å†…å­˜å‡è¡¡ï¼Œå…¼é¡¾åžåä¸Žæ˜¾å­˜æ•ˆçŽ‡ã€‚

![](assets/10_pipeline.assets/10pipeline06.png)

### Hanayo Wave-like Pipeline

Hanayo æ˜¯ä¸€ç§æ³¢æµªå¼æµæ°´çº¿å¹¶è¡Œç­–ç•¥ï¼šå®ƒå°†æ¨¡åž‹åˆ’åˆ†ä¸ºSä¸ªé˜¶æ®µå¹¶å°†å°æ‰¹æ¬¡åˆ†æˆWä¸ªæ³¢ï¼ˆwaveï¼‰ï¼Œä»¥æ³¢æµªå½¢çš„é¡ºåºåœ¨å„é˜¶æ®µäº¤é”™æ‰§è¡Œå‰å‘å’ŒåŽå‘è®¡ç®—ï¼Œèƒ½å¤Ÿå°†æµæ°´çº¿æ°”æ³¡æ¯”ä¾‹é™ä½Žè‡³åŽŸæ¥çš„1/(2W) ä¸”æ— éœ€å¤åˆ¶æ¨¡åž‹ï¼Œä»Žè€Œä¿æŒä¸Žä¸»æµæ–¹æ³•ä¸€è‡´çš„æƒé‡å’Œæ¿€æ´»å†…å­˜å ç”¨ï¼›åŒæ—¶ï¼Œå…¶è½»é‡çº§è¿è¡Œæ—¶å¼•æ“Žå°†è°ƒåº¦é€»è¾‘ä¸Žæ‰§è¡Œå’Œé€šä¿¡ä¼˜åŒ–è§£è€¦ï¼Œæ”¯æŒåœ¨å¤šå¡é›†ç¾¤ä¸Šçµæ´»éƒ¨ç½²ï¼›åœ¨å¯¹GPTå’ŒBERTç±»æ¨¡åž‹ã€æœ€å¤š32å—GPUçš„æµ‹è¯•ä¸­ï¼ŒHanayoç›¸è¾ƒæœ€å…ˆè¿›æ–¹æ¡ˆå®žçŽ°äº†æœ€é«˜30.4%çš„åžåé‡æå‡

![](assets/10_pipeline.assets/10pipeline07.png)

## åˆ†å¸ƒå¼æ¡†æž¶é‡Œçš„PPå®žçŽ°

- æ¨¡åž‹è¿è¡Œå…¥å£ä¸ŽPPé…ç½®ï¼š pretrain_gpt.py mainå‡½æ•°è°ƒç”¨pretrain->get_model,get_modelå‡½æ•°åˆ¤æ–­pipelineçš„åˆ’åˆ†ç­–ç•¥

```
Megatron-LM/pretrain_gpt.py

if __name__ == "__main__":

# Temporary for Transition to Core Datasets
    train_valid_test_datasets_provider.is_distributed = True

# Optionally Enable Inprocess Restart on Pretrain
    pretrain, store = inprocess_restart.maybe_wrap_for_inprocess_restart(pretrain)

    pretrain(
        train_valid_test_datasets_provider,
        model_provider,
        ModelType.encoder_or_decoder,
        forward_step,
        args_defaults={'tokenizer_type': 'GPT2BPETokenizer'},
        extra_args_provider=add_modelopt_args if has_nvidia_modelopt else None,
        store=store,
    )
```

pretrainå‡½æ•°å†…éƒ¨è°ƒç”¨setup_model_and_optimizerå‡½æ•°ï¼Œè¯¥å‡½æ•°å†…éƒ¨è°ƒç”¨get_model

```
Megatron-LM/megatron/training/training.py/def pretrain

# Model, Optimizer, and Learning Rate.
timers('model-and-optimizer-setup', log_level=0).start(barrier=True)
app_metrics['app_build_optimizer_start_time'] = one_logger_utils.get_timestamp_in_ms()
model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
    model_provider, model_type, checkpointing_context=checkpointing_context
)

Megatron-LM/megatron/training/training.py/def setup_model_and_optimizer

def setup_model_and_optimizer(
    model_provider_func,
    model_type,
    no_wd_decay_cond=None,
    scale_lr_cond=None,
    lr_mult=1.0,
    checkpointing_context=None,
):
    """Setup model and optimizer."""
    args = get_args()
    timers = get_timers()
    one_logger = get_one_logger()

    model = get_model(model_provider_func, model_type)
    unwrapped_model = unwrap_model(model)
```

get_modelé€šè¿‡get_argså‡½æ•°æ‹¿åˆ°å¯åŠ¨è„šæœ¬è®¾ç½®çš„è¶…å‚ï¼Œå‚æ•°è®¾ç½®å¦‚å›¾æ‰€ç¤ºï¼š

```python
MODEL_PARALLEL_ARGS = (
	--tensor-model-parallel-size 8
	--pipeline-model-parallel-size 16
)
```

```
Megatron-LM/megatron/training/training.py/def get_model

def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap_with_ddp=True):
    """Build the model."""
    args = get_args()
    args.model_type = model_type

# Build Model.
```

æ­¤å¤„åˆ†ä¸ºä¸¤ç§æƒ…å†µè®¨è®ºï¼Œä»¥ä¸‹æ˜¯å¯ç”¨è™šæ‹Ÿç®¡é“(VPP)çš„æ¨¡åž‹æž„å»ºï¼Œåˆ¤æ–­æ¡ä»¶å¦‚ç¬¬ä¸€ä¸ªifæ‰€ç¤ºã€‚åˆ¤å®šé€»è¾‘æ ‡è®°äº†ï¼šåªæœ‰ç¬¬ä¸€ä¸ªrankåšè¾“å…¥ï¼Œæœ€åŽä¸€ä¸ªrankåšè¾“å‡ºï¼Œåˆ©ç”¨model_provider_funcå‡½æ•°è®¡ç®—å½“å‰Rankè¯¥â€œåˆ‡â€å“ªä¸€æ®µ Transformer å±‚å¹¶å®žä¾‹åŒ–ï¼Œæœ€ç»ˆæŠŠæ‰€æœ‰RankæŒ‰é¡ºåºæ”¾å…¥modelåˆ—è¡¨ï¼Œä¾›åŽé¢çš„æµæ°´çº¿è°ƒåº¦å™¨å¾ªçŽ¯è°ƒç”¨ã€‚

```
Megatron-LM/megatron/training/training.py/def get_model
 
  if (
            mpu.get_pipeline_model_parallel_world_size() > 1
            and args.virtual_pipeline_model_parallel_size is not None
        ):
            if model_type == ModelType.encoder_and_decoder:
                assert (
                    args.encoder_pipeline_model_parallel_size == 0
                ), "Interleaved schedule not supported for model with encoder on separate PP rank"
            model = []
            for i in range(args.virtual_pipeline_model_parallel_size):
# Set pre_process and post_process only after Virtual Rank is Set.
                pre_process = mpu.is_pipeline_first_stage(ignore_virtual=False, vp_stage=i)
                post_process = mpu.is_pipeline_last_stage(ignore_virtual=False, vp_stage=i)
                this_model = model_provider_func(
                    pre_process=pre_process, post_process=post_process, vp_stage=i)
                this_model.model_type = model_type
                this_model.vp_stage = i
                model.append(this_model)
```

å¦åˆ™å¯ç”¨PipeDreamæµæ°´çº¿å¹¶è¡Œï¼Œæ ¹æ®æ¨¡åž‹ç±»åž‹å’Œå¹¶è¡Œåº¦ï¼Œå°†ç¼–ç å™¨å’Œè§£ç å™¨æ¨¡å—åˆç†åœ°æ‹†åˆ†åˆ°ä¸åŒ GPUï¼Œä¿è¯å‰å‘/åå‘ä¼ é€’çš„æ­£ç¡®æ€§ä¸Žé«˜æ•ˆæ€§ã€‚

```
Megatron-LM/megatron/training/training.py/def get_model

else:
            pre_process = mpu.is_pipeline_first_stage()
            post_process = mpu.is_pipeline_last_stage()
            add_encoder = True
            add_decoder = True
            if model_type == ModelType.encoder_and_decoder:
                if mpu.get_pipeline_model_parallel_world_size() > 1:
                    rank = mpu.get_pipeline_model_parallel_rank()
                    first_decoder_rank = args.encoder_pipeline_model_parallel_size
                    world_size = mpu.get_pipeline_model_parallel_world_size()
                    pre_process = rank == 0 or rank == first_decoder_rank
                    post_process = (rank == (first_decoder_rank - 1)) or (rank == (world_size - 1))
                    add_encoder = mpu.is_inside_encoder(rank)
                    add_decoder = mpu.is_inside_decoder(rank)
                model = model_provider_func(
                    pre_process=pre_process,
                    post_process=post_process,
                    add_encoder=add_encoder,
                    add_decoder=add_decoder,
                )
            else:
                model = model_provider_func(pre_process=pre_process, post_process=post_process)
            model.model_type = model_type
        return model
```

- PPæ¨¡åž‹å®žä¾‹åŒ–ï¼šï¼ˆæ²¡æ‰¾å…¨ï¼‰

é€šè¿‡ä¸Šè¿°get_modelå‡½æ•°é‡Œçš„model_provider_funcå‡½æ•°æž„å»ºæ¨¡åž‹å®žä¾‹ï¼Œmodel_provider_funcå¹¶ä¸æ˜¯Megatron-Coreåº“é‡Œä¸€ä¸ªå•ç‹¬çš„å…¨å±€å‡½æ•°ï¼Œè€Œæ˜¯ç”±å„ä¸ªé¢„è®­ç»ƒè„šæœ¬(å¦‚ pretrain_gpt.py)å®šä¹‰å¹¶ä¼ å…¥æ ¸å¿ƒè®­ç»ƒæµç¨‹çš„å›žè°ƒã€‚

```
Megatron-LM/pretrain_gpt.py

def model_provider(
    pre_process=True, post_process=True, vp_stage: Optional[int] = None
) -> Union[GPTModel, megatron.legacy.model.GPTModel]:
```

æž„å»ºGPTModelå®žä¾‹ï¼Œ

```
Megatron-LM/megatron/core/models/gpt/gpt_model.py

class GPTModel(LanguageModule):
    def __init__(â€¦):

# Transformer.
        self.decoder = TransformerBlock(
            config=self.config,
            spec=transformer_layer_spec,
            pre_process=self.pre_process,
            post_process=self.post_process,
            vp_stage=vp_stage,
        )
```

- PPèŽ·å–éœ€è¦æ‰§è¡Œçš„å±‚æ•°ï¼šï¼ˆæ²¡çœ‹æ‡‚ï¼‰

TransformerBlockæ³¨å†Œé€šè¿‡get_num_layers_to_buildè®¡ç®—å½“å‰StageåŒ…å«å‡ ä¸ªTransformer Layer

```
Megatron-LM/megatron/core/transformer/transformer_block.py

def get_num_layers_to_build(config: TransformerConfig, vp_stage: Optional[int] = None) -> int:
    return num_layers_to_build

class TransformerBlockSubmodules:

    def _get_block_submodules(â€¦):

    if isinstance(spec, TransformerBlockSubmodules):
            return spec

# ModuleSpec here is Generally Assumed to Be for a Transformer Layer that
# Is Implemented in `transformer_layer.py` or if it Subclasses
# `BaseTransformerLayer` From the `transformer_layer.py` File.
        elif isinstance(spec, ModuleSpec):
            if issubclass(spec.module, TransformerBlock):
                return spec.submodules
            elif issubclass(spec.module, BaseTransformerLayer):
                num_layers = get_num_layers_to_build(config, vp_stage)
                return TransformerBlockSubmodules(
                    layer_specs=[spec] * num_layers, layer_norm=LayerNormImpl
                )
            else:
                raise Exception(f"specialize for {spec.module.__name__}.")
        else:
            raise Exception(f"specialize for {type(spec).__name__}.")
```

åœ¨ GPT æ¨¡åž‹è¿è¡Œç¤ºä¾‹ä¸­æ¯ä¸ª Stage build_layer çš„ä¸ªæ•°ä¸º number_lyaer = L / PP_num

```
Megatron-LM/megatron/core/transformer/transformer_block.py

class TransformerBlock(MegatronModule):
    """Transformer class."""

    def __init__ï¼ˆ
        self.num_layers_per_pipeline_rank = len(self.layers)

    def _build_layers(self):
# Transformer Layers.
# @jcasper Can We Improve how We Deal with layer_number?
# Currently It's only Used in CoreAttention?
# If self.apply_query_key_layer_scaling:
# Coeff = self.layer_number
# self.norm_factor *= Coeff
        def build_layer(layer_spec, layer_number):
            global_layer_number = layer_number + get_transformer_layer_offset(
                self.config, self.vp_stage
            )  # 1-based index
            if self.config.heterogeneous_block_specs:
                layer_config = self.config.get_config_for_layer(global_layer_number)
            else:
                layer_config = self.config
```

- æ‰§è¡ŒPPè®­ç»ƒï¼š

GPTè®­ç»ƒè°ƒç”¨ pretrain -> train -> train_stepï¼Œæ‰§è¡Œä¸€ä¸ª iteration, train_stepå‡½æ•°é€šè¿‡get_forward_backward_fun()å‡½æ•°è¿›å…¥schedulers.pyæ¨¡å—ï¼Œå¹¶æ ¹æ®å½“å‰çš„ PP æ¨¡å¼è¿”å›žforward_backward_pipelining_with_interleavingæ‰§è¡Œå‰å‘å’Œåå‘è®¡ç®—

```
Megatron-LM/megatron/training/training.py

def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_scheduler, config):
    """Single training step."""
            â€¦
# Forward Pass.
        forward_backward_func = get_forward_backward_func()
        losses_reduced = forward_backward_func(
            forward_step_func=forward_step_func,
            data_iterator=data_iterator,
            model=model,
            num_microbatches=get_num_microbatches(),
            seq_length=args.seq_length,
            micro_batch_size=args.micro_batch_size,
            decoder_seq_length=args.decoder_seq_length,
            forward_only=False,
            adjust_tensor_shapes_fn=adjust_tensor_shapes_fn,
        )

Megatron-LM/megatron/core/pipeline_parallel/schedules.py

def get_forward_backward_func():
    pipeline_model_parallel_size = parallel_state.get_pipeline_model_parallel_world_size()
    if pipeline_model_parallel_size > 1:
        if parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None:
            forward_backward_func = forward_backward_pipelining_with_interleaving
        else:
            forward_backward_func = forward_backward_pipelining_without_interleaving
    else:
        forward_backward_func = forward_backward_no_pipelining
    return forward_backward_func
```

- NPU0æ‰§è¡Œstage0ï¼ˆä¸æ¸…æ™°ï¼‰

æ‰§è¡Œ Forward è®¡ç®—ï¼Œé€‰æ‹© forward_backward_pipelining_without_interleavingæ¨¡å¼ (ä»¥Pipeline 1F1Bä¸ºä¾‹ï¼Œå³PipeDream) å…ˆå…³é—­æ¢¯åº¦æ›´æ–°ï¼Œç­‰æ‰€æœ‰çš„microbatchæ‰§è¡Œå®Œæ¯•æ‰æ›´æ–°æ¢¯åº¦ã€‚è¿‡ç¨‹å¦‚å›¾æ‰€ç¤ºï¼š

![](assets/10_pipeline.assets/10pipeline10.png)


éƒ¨åˆ†ä»£ç å±•ç¤ºï¼š å…¶ä¸­ï¼šnum_microbatchesï¼šæ€»çš„ micro batch ä¸ªæ•°ã€‚num_warmup_microbatchesï¼šå½“å‰rank warmupé˜¶æ®µéœ€è¦è®¡ç®—ï¼Œç›´åˆ°1F1B çš„microbatchçš„ä¸ªæ•°ã€‚ num_microbatches_remainingï¼šå½“å‰rankè¿˜å‰©ä¸‹å¤šå°‘ä¸ªmicrobatchæ‰§è¡Œæ‰åˆ°1F1Bé˜¶æ®µï¼Œå³num_microbatches - num_warmup_microbatchesã€‚

```
Megatron-LM/megatron/core/pipeline_parallel/schedules.py

def forward_backward_pipelining_without_interleaving(
    â€¦
    micro_batch_size: int,
    â€¦
):
    â€¦
    disable_grad_sync()

# Compute Number of Warmup Microbatches.
    num_warmup_microbatches = (
        parallel_state.get_pipeline_model_parallel_world_size()
        - parallel_state.get_pipeline_model_parallel_rank()
        - 1
    )
    num_warmup_microbatches = min(num_warmup_microbatches, num_microbatches)
    num_microbatches_remaining = num_microbatches - num_warmup_microbatches
```

- NPU0å®Œæˆå‰å‘è®¡ç®—ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š NPU0åœ¨stage0é˜¶æ®µæ²¡æœ‰å…¶å®ƒçš„Stageæ¿€æ´»è¾“å…¥ï¼Œå› æ­¤å¿½ç•¥recv_forward () å‡½æ•°ï¼Œforward_stepè°ƒç”¨ forward_step_func çœŸæ­£è°ƒç”¨æ¨¡åž‹æ‰§è¡Œï¼š

![](assets/10_pipeline.assets/10pipeline12.png)


```
Megatron-LM/megatron/core/pipeline_parallel/schedules.py

# Run Warmup forward Passes.
    for i in range(num_warmup_microbatches):
# Decide to Checkpoint All Layers' Activations of the Current Micro-batch
        if max_outstanding_backprops is not None:
            checkpoint_activations_microbatch = (
                i % max_outstanding_backprops
                >= config.num_microbatches_with_partial_activation_checkpoints
            )
        else:
            checkpoint_activations_microbatch = None

        input_tensor = recv_forward(
            recv_tensor_shapes, config, parallel_state.is_pipeline_first_stage()
        )
        output_tensor, num_tokens = forward_step(
            forward_step_func,
            data_iterator,
            model,
            num_microbatches,
            input_tensor,
            forward_data_store,
            config,
            collect_non_loss_data,
            checkpoint_activations_microbatch,
            check_first_val_step(first_val_step, forward_only, i == 0),
            current_microbatch=i,
            encoder_decoder_xattn=encoder_decoder_xattn,
        )

        def forward_step(â€¦)
            â€¦
            if config.enable_autocast:
                context_manager = torch.autocast("cuda", dtype=config.autocast_dtype)
            else:
                context_manager = contextlib.nullcontext()
            with context_manager:
                if checkpoint_activations_microbatch is None:
                    output_tensor, loss_func = forward_step_func(data_iterator, model)
                else:
                    output_tensor, loss_func = forward_step_func(
                        data_iterator, model, checkpoint_activations_microbatch
                    )
```

- NPU0å‰å‘ä¼ é€’æ¿€æ´»ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š


![](assets/10_pipeline.assets/10pipeline13.png)


- NPU0ä¸Šè¾“å‡ºStage0 output_tensoråŽsend_forwardå‘é€ç»™ä¸‹ä¸€ä¸ªStageï¼Œé€šè¿‡P2P_communication.send_forwardå‘é€output_tensorï¼Œé€šè¿‡torch.distributed.P2POpå¼‚æ­¥send output_tensorï¼Œæœ€åŽè°ƒç”¨ torch.cuda.synchronize() æ‰§è¡ŒåŒæ­¥

```
Megatron-LM/megatron/core/pipeline_parallel/schedules.py

        send_forward(
            output_tensor, send_tensor_shapes, config, parallel_state.is_pipeline_last_stage()
        )

        def send_forward(output_tensors, tensor_shapes, config, is_last_stage):
    """Wrapper for p2p_communication.send_forward used with non-interleaving schedule."""
    if not isinstance(output_tensors, list):
        output_tensors = [output_tensors]
    for output_tensor, tensor_shape in zip(output_tensors, tensor_shapes):
        if tensor_shape is None:
            continue
        p2p_communication.send_forward(output_tensor, config, is_last_stage)

Megatron-LM/megatron/core/pipeline_parallel/p2p_communication.py

      if wait_on_reqs and len(reqs) > 0:
        for req in reqs if isinstance(reqs, list) else reqs.values():
            req.wait()
        reqs = None

    if (
        (config.batch_p2p_comm and config.batch_p2p_sync)
# The Lists below Have a Size > 1 only when ETP â‰  DTP,
# Meaning This Synchronization is Required when ETP â‰  DTP.
        or len(tensor_recv_prev_list) > 1
        or len(tensor_recv_next_list) > 1
    ):
# To Protect against Race Condition when Using batch_isend_irecv().
# User Should Assert that We Have a Modern Enough PyTorch to not Need This
        torch.cuda.synchronize()    
```

- NPU0ç»§ç»­è®¡ç®—ï¼š NPU0ç»§ç»­æ‰§è¡Œforward_step, Stage0å‰å‘è®¡ç®—å¾—åˆ°ç¬¬äºŒä¸ªoutput_tensor, åˆ©ç”¨sedn_forward_recv_backwardå‡½æ•°å‘é€output_tensorç­‰å¾…backwardï¼Œè¿›å…¥1F1BçŠ¶æ€ï¼Œé€šè¿‡send_backward_recv_backwardåº•å±‚è¯•ä¸‹é€šè¿‡P2PPpå¼‚æ­¥ï¼Œsend output_tesnorï¼Œä¸”å¼‚æ­¥recv tensor_recv_nextï¼Œæœ€åŽè°ƒç”¨synchronize () ç­‰å¾…recv backwardï¼ŒNPU0è¿›å…¥ç­‰å¾…çŠ¶æ€ã€‚

![](assets/10_pipeline.assets/10pipeline17.png)


```
Megatron-LM/megatron/core/pipeline_parallel/schedules.py

def forward_backward_pipelining_without_interleaving(â€¦):
 def enable_grad_sync():
# Run Warmup forward Passes.
    for i in range(num_warmup_microbatches):
# Decide to Checkpoint All Layers' Activations of the Current Micro-batch
        if max_outstanding_backprops is not None:
            checkpoint_activations_microbatch = (
                i % max_outstanding_backprops
                >= config.num_microbatches_with_partial_activation_checkpoints
            )
        else:
            checkpoint_activations_microbatch = None

        input_tensor = recv_forward(
            recv_tensor_shapes, config, parallel_state.is_pipeline_first_stage()
        )
        output_tensor, num_tokens = forward_step(
            forward_step_func,
            data_iterator,
            model,
            num_microbatches,
            input_tensor,
            forward_data_store,
            config,
            collect_non_loss_data,
            checkpoint_activations_microbatch,
            check_first_val_step(first_val_step, forward_only, i == 0),
            current_microbatch=i,
            encoder_decoder_xattn=encoder_decoder_xattn,
        )
```

- NPU1è¿›è¡Œå‰å‘è®¡ç®—ï¼š å…¶è¿‡ç¨‹åŒGPU0ä¸€è‡´ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š num_warmup_microbatches=0ï¼Œè¿›å…¥1F1BçŠ¶æ€ï¼Œnum_microbatches_remaining=3ï¼Œrecv_forward è°ƒç”¨ P2POp å¼‚æ­¥recvï¼ŒNPU1 æœ€åŽè°ƒç”¨synchronize () æ‰§è¡ŒåŒæ­¥ç­‰å¾… NPU0 Stage0 è¾“å‡ºï¼Œä»Žè€Œä¿è¯NPU0 to NPU1 çš„æ‰§è¡Œé¡ºåºã€‚ 

![](assets/10_pipeline.assets/10pipeline14.png)


NPU1 recv_forwardç­‰å¾…NPU0 Stage0å‘é€intput_tensoråŽNPU1 forward_stepè®¾ç½®iNPUt_tensorï¼Œå®žçŽ°NPU0&NPU1äº¤æ¢è¾“å…¥è¾“å‡ºNPU1è¿›å…¥1F1Bå¾ªçŽ¯ï¼Œforward_step_funcè°ƒç”¨GPTModelæ‰§è¡Œå‰å‘è®¡ç®—ã€‚NPU1 ä¸ŠTransformerBlock æ‰§è¡Œç¬¬ä¸€ä¸ª Stageï¼ŒPre_process=Falseï¼Œå³ä¸ä¼šæŠŠiNPUt_embeddingsä½œä¸ºransformerçš„è¾“å…¥ï¼Œä½¿ç”¨NPU0 Stage0è¾“å…¥çš„iNPUt_tensorä½œä¸ºè¾“å…¥æ‰§è¡Œå¾—åˆ°output tensorã€‚

![](assets/10_pipeline.assets/10pipeline15.png)

```
Megatron-LM/megatron/core/transformer/transformer_block.py

class TransformerBlock(MegatronModule):
    """Transformer class."""

    def __init__(
        self,
        config: TransformerConfig,
        spec: Union[TransformerBlockSubmodules, ModuleSpec],
        post_layer_norm: bool = True,
        pre_process: bool = False,
        post_process: bool = True,
        model_comm_pgs: ModelCommProcessGroups = None,
        vp_stage: Optional[int] = None,
    ):
        super().__init__(config=config)

        self.submodules = _get_block_submodules(config, spec, vp_stage)
        self.post_layer_norm = post_layer_norm
        self.pre_process = pre_process
        self.post_process = post_process
        self.vp_stage = vp_stage

    def forward(â€¦):
        if not self.pre_process:
# See set_input_tensor()
            hidden_states = self.input_tensor
```

ç¤ºä¾‹ä¸­NPU1 Stage1æ˜¯æœ€åŽä¸€å±‚Staegeï¼Œå› æ­¤post_process=True, æ‰§è¡Œ is_pipeline_last_stageè®¡ç®—GPTæ¨¡åž‹çš„output_tensorå’Œlossã€‚

![](assets/10_pipeline.assets/10pipeline16.png)


```
Megatron-LM/megatron/core/transformer/transformer_block.py

class TransformerBlock(MegatronModule):
    """Transformer class."""

    def __init__(
        self,
        config: TransformerConfig,
        spec: Union[TransformerBlockSubmodules, ModuleSpec],
        post_layer_norm: bool = True,
        pre_process: bool = True,
        post_process: bool = True,
        model_comm_pgs: ModelCommProcessGroups = None,
        vp_stage: Optional[int] = None,
    ):

Megatron-LM/megatron/core/pipeline_parallel/schedules.py

def forward_step(â€¦)
    â€¦
   if parallel_state.is_pipeline_last_stage(ignore_virtual=False, vp_stage=vp_stage):
        if not collect_non_loss_data:
            outputs = loss_func(output_tensor)
            if len(outputs) == 3:
                output_tensor, num_tokens, loss_reduced = outputs
                if not config.calculate_per_token_loss:
                    output_tensor /= num_tokens
                    output_tensor /= num_microbatches
            else:
# Preserve Legacy Loss Averaging Behavior (ie, over the Number of microbatches)
                assert len(outputs) == 2
                output_tensor, loss_reduced = outputs
                output_tensor *= parallel_state.get_context_parallel_world_size()
                output_tensor /= num_microbatches
            forward_data_store.append(loss_reduced)
        else:
            data = loss_func(output_tensor, non_loss_data=True)
            forward_data_store.append(data)
```

- NPU1åå‘æ‰§è¡ŒStage1ï¼š æ‰§è¡Œå®Œforward_stepåŽæ‰§è¡Œbackward_stepå¾—åˆ°iNPUt_tensor_gradï¼Œå¹¶è¿›å…¥1F1BçŠ¶æ€ï¼Œæ‰§è¡Œsend_backward_recc_forward->_communication->å¼‚æ­¥å‘é€iNPUt_tensor_gradç»™NPU0å¹¶ç­‰å¾…NPU0å‘é€ä¸‹ä¸€ä¸ªMB forwardç»“æžœã€‚


![](assets/10_pipeline.assets/10pipeline18.png)

```
Megatron-LM/megatron/core/pipeline_parallel/schedules.py

def forward_backward_pipelining_without_interleaving(â€¦):
# Enable Grad Sync for the Last Microbatch in the Batch if the Full
# Backward Pass Completes in the 1F1B Stage.
    if num_warmup_microbatches == 0 and last_iteration:
        if config.grad_sync_func is None or rank == 0:
            enable_grad_sync()

    input_tensor_grad = backward_step(
        input_tensor, output_tensor, output_tensor_grad, model_type, config
    )

    if last_iteration:
        input_tensor = None
        send_backward(
            input_tensor_grad,
            recv_tensor_shapes,
            config,
            parallel_state.is_pipeline_first_stage(),
        )
    else:
        input_tensor = send_backward_recv_forward(
            input_tensor_grad,
            recv_tensor_shapes,
            config,
            parallel_state.is_pipeline_first_stage(),
        ) 

def send_backward_recv_forward(input_tensor_grads, tensor_shapes, config, is_first_stage):
    """Wrapper for p2p_communication.send_backward_recv_forward used
    with non-interleaving schedule."""
    if not isinstance(input_tensor_grads, list):
        input_tensor_grads = [input_tensor_grads]
    input_tensors = []
    for input_tensor_grad, tensor_shape in zip(input_tensor_grads, tensor_shapes):
        if tensor_shape is None:
            input_tensors.append(None)
            continue
        input_tensor = p2p_communication.send_backward_recv_forward(
            input_tensor_grad, tensor_shape, config, is_first_stage
        )
        input_tensors.append(input_tensor)
    return input_tensors
```

- NPU0åå‘æ‰§è¡ŒStage0ï¼š NPU0 Srage0ç­‰å¾…send_backward_recv_forwardè¢«å”¤é†’åŽèŽ·å¾—NPU1 Staege1å‘é€çš„output_tensor_gradï¼ŒNPU0 Stage0æ‰§è¡Œbackward_stepè¾“å‡ºintput_tensor_gradï¼ŒNPU0è®¡å…¥1F1BçŠ¶æ€ï¼ŒNPU0 num_warmup_mbs=1, num_mbs_remaining=2ï¼Œè¿›å…¥1F1Bå¾ªçŽ¯ï¼Œæ‰§è¡Œforward_stepæ‰§è¡ŒStarge1å‰å‘è®¡ç®—å¾—åˆ°output_tensor (Forward 3)ï¼Œæ‰§è¡Œsend_forward_recv_backwardå‘é€output_tensorç­‰å¾…backwardï¼Œå¼‚æ­¥recv tensor_recv_nextï¼Œè°ƒç”¨synnchronize () åŒæ­¥ç­‰å¾…backwardï¼ŒNPU0 è¿›å…¥ç­‰å¾…çŠ¶æ€ã€‚


![](assets/10_pipeline.assets/10pipeline19.png)


```
Megatron-LM/megatron/core/pipeline_parallel/schedules.py

# Run 1F1B in Steady State.
    for i in range(num_microbatches_remaining):
        last_iteration = i == (num_microbatches_remaining - 1)

# Decide to Checkpoint All Layers' Activations of the Current Micro-batch
        if max_outstanding_backprops is not None:
            checkpoint_activations_microbatch = (
                (i + num_warmup_microbatches) % max_outstanding_backprops
            ) >= config.num_microbatches_with_partial_activation_checkpoints
        else:
            checkpoint_activations_microbatch = None

        output_tensor, num_tokens = forward_step(â€¦)
        total_num_tokens += num_tokens

        if forward_only:
            send_forward(
                output_tensor, send_tensor_shapes, config, parallel_state.is_pipeline_last_stage()
            )

            if not last_iteration:
                input_tensor = recv_forward(
                    recv_tensor_shapes, config, parallel_state.is_pipeline_first_stage()
                )

        else:
            output_tensor_grad = send_forward_recv_backward(
                output_tensor, send_tensor_shapes, config, parallel_state.is_pipeline_last_stage()
            )

Megatron-LM/megatron/core/pipeline_parallel/p2p_communication.py

  if recv_prev:
        if config.pipeline_dtype is None:
            raise RuntimeError("pipeline_dtype must be provided if recv_prev is True")
        if tensor_shape is None:
            raise RuntimeError(
                "tensor_shape must be specified if recv_prev is True. "
                "Common tensor_shape is (seq_length, micro_batch_size, hidden_size)"
            )
        tensor_recv_prev_func = create_tensor_recv_prev

    if recv_next:
        if config.pipeline_dtype is None:
            raise RuntimeError("dtype must be provided if recv_next is True")
        if tensor_shape is None:
            raise RuntimeError(
                "tensor_shape must be specified if recv_next is True. "
                "Common tensor_shape is (seq_length, micro_batch_size, hidden_size)"
            )
        tensor_recv_next_func = create_tensor_recv_next
```

- NPU1åå‘æ‰§è¡ŒStage1ï¼š åŒç†ï¼ŒNPU1 Stage1ä¸Šæ‰§è¡Œsend_backward_recv_forwardåŒæ­¥ç­‰å¾…æ”¶åˆ°NPU0 Stge0å‘é€iNPUt_tensorï¼ˆForward 2ï¼‰, NPU1 Stage1å°†iNPUt_tensorï¼ˆForward 2ï¼‰ä½œä¸ºTransformerBlockæ‰§è¡Œforward_step, å¾—åˆ°è¾“å‡ºoutput_tensorã€‚


![](assets/10_pipeline.assets/10pipeline19%201.png)

NPU0æ‰§è¡ŒStage0 NPU0ç­‰å¾…send_forward_recv_backwardæ‰§è¡ŒNPU1è¾“å‡ºoutput_grad(B2)ï¼Œæ‰§è¡Œbackward_stepè¾“å‡ºiNPUt_tensor_gradï¼ˆB2ï¼‰ï¼ŒNPU0 num_warmup_mbs=1, num_mbs_remaing=2, i=2ï¼Œé€€å‡º 1F1Bï¼Œè¿›å…¥cooldown backwrd pass enable_grad_syncæ‰“å¼€æ¨¡åž‹æ¢¯åº¦æ›´æ–°ï¼Œrecv_backwardç­‰å¾…NPU1å‘é€æœ€åŽä¸€ä¸ª mbs çš„backwardï¼ˆB3ï¼‰ï¼ŒNPU0 å‡†å¤‡æ›´æ–°æ¨¡åž‹çš„æ¢¯åº¦å’Œå‚æ•°ã€‚


![](assets/10_pipeline.assets/10pipeline20.png)

NPU1æ‰§è¡ŒStage1 NPU1 Stage1æ‰§è¡Œsend_backward_recv_forwardåŒæ­¥ç­‰å¾…iNPUt (F3), NPU1 num_warmup_mbs=0ï¼Œnum_mbs_remaining=3ï¼Œè¿›å…¥1F1Bå¾ªçŽ¯, å°†NPU0 Stage0å‘é€ iNPUt (F3) ä½œä¸ºTransformerBlockçš„iNPUtè®¡ç®—å‰å‘, forward_step () è¾“å‡ºoutput (F3) æ‰§è¡Œbackward_step () å¾—åˆ°iNPUt_tensor_grad (B3), send_backward () å¼‚æ­¥å‘é€iNPUt_tesnor_grad (B3) ç»™NPU0ã€‚

![](assets/10_pipeline.assets/10pipeline21.png)


NPU0æ‰§è¡ŒStage0åŽï¼Œæ‰§è¡Œå®Œå®Œæ•´çš„iteration NPU0ç­‰å¾…cooldown backwardçš„recv_backward () èŽ·å¾—NPU1è¾“å‡º (B3)ï¼Œæ‰§è¡Œ backward_step () è¾“å‡ºiNPUt_tensor_grad (B3)ï¼Œforward_backward_func () è¿”å›žLOSSï¼Œenable_grad_sync () ç´¯åŠ æ›´æ–°æ¨¡åž‹æ¢¯åº¦ï¼Œfinalize_model_grads_func () æ›´æ–°æ¨¡åž‹å‚æ•°ã€‚

![](assets/10_pipeline.assets/10pipeline22.png)

```
Megatron-LM/megatron/core/pipeline_parallel/schedules.py

def get_forward_backward_func():
  pipeline_model_parallel_size = parallel_state.get_pipeline_model_parallel_world_size()
    if pipeline_model_parallel_size > 1:
        if parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None:
            forward_backward_func = forward_backward_pipelining_with_interleaving
        else:
            forward_backward_func = forward_backward_pipelining_without_interleaving
    else:
        forward_backward_func = forward_backward_no_pipelining
    return forward_backward_func

def forward_backward_pipelining_without_interleaving(â€¦)
    def enable_grad_sync():
        output_tensor_grad = recv_backward(
                        send_tensor_shapes, config, parallel_state.is_pipeline_last_stage()
                    )

                    input_tensor_grad = backward_step(
                        input_tensor, output_tensor, output_tensor_grad, model_type, config
                    )

                    send_backward(
                        input_tensor_grad,
                        recv_tensor_shapes,
                        config,
                        parallel_state.is_pipeline_first_stage(),
                    )

# Launch Any Remaining Grad Reductions.
                if no_sync_context is not None:
                    enable_grad_sync()
                    if config.grad_sync_func is not None:
                        config.grad_sync_func(model.parameters())

            if config.finalize_model_grads_func is not None and not forward_only:

# If defer_embedding_wgrad_compute is Enabled We Need to Do the
# Weight Gradient GEMM's Here.
                finish_embedding_wgrad_compute(config, embedding_module)

# Finalize Model Grads (perform Full Grad All-reduce / Reduce-scatter for
# Data Parallelism, Layernorm All-reduce for Sequence Parallelism, and
# Embedding All-reduce for Pipeline parallelism).
                config.finalize_model_grads_func(
                    [model], total_num_tokens if config.calculate_per_token_loss else None
                )

            if config.timers is not None:
                config.timers('forward-backward').stop()

            if hasattr(config, 'enable_cuda_graph') and config.enable_cuda_graph:
                create_cudagraphs()

            return forward_data_store
```
