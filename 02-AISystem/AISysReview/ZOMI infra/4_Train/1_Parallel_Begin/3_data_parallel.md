---
dateCreated: 2025-08-09
dateModified: 2025-08-09
---
# 数据并行

#### 数据并行的定义与类型

数据并行是一种将单机训练任务扩展到多设备并行计算的算法，理想情况下可实现与设备数量成正比的加速效果。其常见类型包括数据并行（DP）、分布式数据并行（DDP）、完全分片的数据并行（ZeRO）、异步数据并行等。其中，DDP 因效率更高、更易优化且能与其他并行算法结合，是优先选择的方案；ZeRO 等扩展类型将在后续详述。

#### 数据并行（DP）

- **运行特点**：仅支持单台机器，采用单进程、多线程模式，将原本在单一设备上进行的数据训练过程，扩展到多个设备并行训练。在某一设备上随机初始化模型和优化器后，就可进行数据并行的训练。
- **训练步骤**：
    1. **前向传播**：将 mini-batch 数据平均分配到各设备，复制模型和优化器至所有设备（确保一致性），各设备同时基于本地数据和模型执行前向传播。
    2. **损失计算与反向传播**：各设备独立计算损失并反向传播，再将梯度传递到某一设备进行累加。
    3. **参数更新**：用累加后的梯度更新模型参数和优化器状态，更新结果在下一轮前向传播中复制到所有设备。
- **缺陷**：
    - **语言层面**：受 Python GIL（全局解释器锁）限制，单进程多线程难以充分利用多设备资源，存在 CPU 性能瓶颈。
    - **算法层面**：梯度累积和参数更新集中在单个设备，导致该设备负载过高而其他设备空闲，资源浪费严重；若 mini-batch 设置过小，会降低设备内并行度，甚至因通信开销导致训练速度慢于单机。

![](03.data_parallel01.png)

#### 分布式数据并行（DDP）

- **核心优势**：是当前应用最广的并行算法之一，综合多种优化，扩展性良好（ZeRO 即为其内存高效扩展版本）。
- **关键改进**：
    1. 采用**多进程并行**，规避 Python GIL 限制，支持多台机器联网扩展，提升分布式规模和效率。
    2. 优化通信机制，如使用分桶 **Ring-AllReduce** 算法进行集合通信（细节见 “集合通信” 章节）。
    3. 设备负载更均衡，避免单一设备承担额外工作的情况。

### DDP 基本流程与异步数据并行解读总结

#### DDP（分布式数据并行）基本流程

- **初始化**：程序启动与设备数量相等的进程，每个进程对应一个训练脚本副本；主进程将设备 0 的模型复制到其他所有设备，确保各设备的模型和优化器完全一致。
- **训练过程**：
    1. **前向传播**：各设备获取独立且完整的 mini-batch 数据，同时基于本地数据执行前向传播。
    2. **损失计算与反向传播**：各设备独立计算损失并进行反向传播，**反向传播与梯度更新同步进行**—— 局部梯度就绪后，通过分桶 Ring-AllReduce 等集合通信算法在所有进程中取平均值，再用全局梯度更新模型参数和优化器状态（梯度一致性保证各设备模型一致，避免收敛问题）。
（注：计算与通信的重叠机制将在后续章节详述；异步数据并行中模型不一致的情况会导致收敛问题，但能提升迭代速度和设备利用率，后续会进一步讨论。）

#### 同步与异步数据并行对比

前面的介绍都是基于**同步的数据并行**的，同步的数据并行特别适用于计算资源相对均衡的情况。在同步数据并行中，每个设备都处理数据的一个子集，并独立地计算梯度。在每次迭代中，所有设备都将它们的梯度汇总，并通过一致的规则来更新模型参数。这样，所有设备上的模型都保持一致，不会出现不同步的情况。由于所有设备在每个训练步骤中都执行相同的更新操作，模型的收敛性更容易得到保证。且所有设备都参与到梯度更新的计算中，整体计算效率也相对较高。此外，同步数据并行还易于实现，因为所有设备的操作都是同步的，不需要复杂的同步机制。



**异步的数据并行**可以在一定程度上解决这些问题。在异步数据并行中，不同设备的计算过程相互独立，不再需要等待其他设备完成计算。每个设备都按照自己的速度进行前向和反向传播，随时将计算得到的梯度更新到模型参数中。这样，快速的设备不再受到慢速设备的影响，整体计算效率得到提高。异步数据并行的优点之一是它可以充分利用集群中每个设备的计算能力，快速的设备不会受到慢速设备的影响，从而提高了整体的训练速度。此外，由于每个设备都独立地进行计算和参数更新，异步数据并行也具有较好的扩展性，能够适应不同规模的集群和不同数量、类型的设备。

但是异步数据并行也存在一些挑战。由于计算过程是异步的，可能会出现梯度更新之间的竞争条件，需要采取一些机制来解决，如：**参数服务器**[[3]] ( https://github.com/Infrasys-AI/AIInfra/blob/main/04Train/01ParallelBegin/03.data_parallel.md#ref3 )。同时由于计算过程不再同步，模型的收敛性可能会受到影响，需要通过调整学习率或者采用一些优化算法来弥补。

|类型|核心特点|优势|局限性|
|---|---|---|---|
|**同步数据并行**|所有设备同步执行，每次迭代汇总梯度并按统一规则更新参数，模型参数始终一致|- 收敛性易保证（设备参数同步更新）<br>- 整体计算效率较高（所有设备参与梯度计算）<br>- 实现简单（无需复杂同步机制）|- 受 “木桶效应” 影响：整体效率取决于最慢设备，设备故障或性能差异会拖慢进度  <br>- 设备过多时，集合通信时间可能成为瓶颈，限制扩展性|
|**异步数据并行**|设备计算过程独立，无需等待其他设备，各自按速度执行前向 / 反向传播，随时用本地梯度更新参数|- 提升整体效率：快速设备不受慢速设备拖累  <br>- 扩展性更好：适应不同规模集群及异构设备|- 模型参数可能不一致，易导致收敛问题  <br>- 需处理参数更新冲突，实现复杂度高于同步模式|

（同步数据并行适用于计算资源均衡的场景，异步数据并行则可缓解同步模式中的效率与扩展性问题，但需权衡收敛性。）

## 数据并行的开源实现

- [DeepSpeed](https://github.com/microsoft/DeepSpeed)：微软推出的分布式训练框架
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)：英伟达推出的分布式训练框架
- [ColossalAI](https://github.com/hpcaitech/ColossalAI)：潞晨科技推出的分布式训练框架
- [Horovod](https://github.com/horovod/horovod)：LF AI & Data Foundation 推出的分布式训练框架
- [BytePS](https://github.com/bytedance/byteps)：字节跳动推出的分布式训练框架
- [PyTorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)、[TensorFlow](https://TensorFlow.%E8%B0%B7%E6%AD%8C.cn/guide/distributed_training?hl=zh-cn) 提供的数据并行接口
