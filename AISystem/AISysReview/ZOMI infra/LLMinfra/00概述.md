本章内容聚焦大模型领域，全面深入地探讨了 Scaling Law 在不同场景如标准、推理时间、并行及扩散等方面的解读，详尽阐述了大模型从训练到推理的全流程，涵盖训练业务流与软硬件栈的各个环节，以及推理业务流的优化策略。同时，剖析了 AI 系统与大模型系统在通用性、资源需求和软件栈变化上的显著区别，并展望了大模型系统的未来发展趋势，包括技术演进、应用场景爆发以及算力底座和生态的持续升级等多方面内容。


## 简介

一是对 Scaling Law 的解读，涉及经典 Scaling Law 在不同场景下的应用，如 Standard Scaling、Inference Time Scaling、Parallel Scaling 和 Diffusion Scaling 等相关研究。

二是大模型训练推理全流程，涵盖了训练业务流与训练软硬件栈，包括并行、加速，数据、模型结构、预训练、后训练，微调，评估等环节，以及推理业务流与推理软硬件栈，像加速、优化、长序列、输出采样、压缩、量化等方面。

三是 AI 系统与大模型系统的区别，从通用性、资源需求（计算、网络、存储）以及软件栈变化（芯片使能、框架、分布式并行、三方套件）三个维度进行了阐述。

四是大模型系统未来发展趋势，探讨了大模型时代的现状与意义，如 Scaling Law 突破、全球竞争格局、产业价值等，分析了技术演进趋势，包含智能体崛起、多模态深度融合、轻量化与高效架构等，展望了应用场景爆发，如生成式重构、工业、医疗、教育等领域，还提到了算力底座持续升级（分布式智算中心发展）以及生态持续构建升级（国产大模型、国产软硬件栈）等情况。