近年的工作中，对于硬件加速器的优化有着各种各样的角度，举几个学术界热门的工作来说，TPU从处理单元阵列（PE Array）层次出发，将Systolic Array架构及其数据流引入DNN加速器中；Eyeriss同样对数据流进行优化，提出了Row Stationary数据流以提升能效；而在电路层次的优化中，打破冯-诺依曼结构的存内计算（In-memory Computing）无疑是最具代表性的方向之一；除此之外，还有许多学者从编译器角度出发，针对不同的硬件target进行优化。

# 压缩算法
## 贝叶斯压缩深度学习
https://blog.csdn.net/weixin_39373480/article/details/89425040



# 模型量化
# 压缩之量化
https://blog.csdn.net/jinzhuojun/article/details/106955059
使用半结构化 (2:4) 稀疏性加速神经网络训练pytorch
https://pytorch.ac.cn/blog/accelerating-neural-network-training/
利用 NVIDIA 安培结构和 NVIDIA TensorRT 加速稀疏推理
https://developer.nvidia.com/zh-cn/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/

# 网络稀疏性
神经网络稀疏综述
https://www.cnblogs.com/sasasatori/p/17809829.html
https://mp.weixin.qq.com/s/Zbp0r9s2Pj5fukxKByaf3g


https://www.bilibili.com/video/BV1ht4y1P73i/?spm_id_from=333.999.0.0&vd_source=bc07d988d4ccb4ab77470cec6bb87b69