---
dateCreated: 2025-08-15
dateModified: 2025-08-15
---
# Deepseek

通信与计算融合

通过算法、框架和硬件的协同设计，克服了跨节点 MoE 训练中的通讯瓶颈，近乎完全的计算/通信重叠，主要在 3.2 节阐述，

DeepSeek-V 3 的训练得到了 HAI-LLM 框架的支持，这是一个由我们的工程师从头开始构建的高效且轻量级的训练框架。总体而言，DeepSeek-V 3 采用了 16 路流水线并行（Pipeline Parallelism, PP）（Qi et al., 2023 a）、跨越 8 个节点的 64 路专家并行（Expert Parallelism, EP）（Lepikhin et al., 2021），以及 ZeRO-1 数据并行（Data Parallelism, DP）（Rajbhandari et al., 2020）。

1. 训练框架设计了 DualPipe 算法，减少了流水线气泡

DualPipe 的核心思想是在一对独立的前向和反向块中重叠计算和通信。具体来说，我们将每个块分为四个部分：**注意力（attention）**、**全对全分发（all-to-all dispatch）**、**MLP** 和 **全对全合并（all-to-all combine）**。特别地，对于反向块，注意力和 MLP 进一步分为两部分：**输入的反向传播** 和 **权重的反向传播**，类似于 ZeroBubble（Qi et al., 2023 b）中的设计。此外，我们还有一个 **PP（Pipeline Parallelism）通信部分**。如图 4 所示，对于一对前向和反向块，我们重新排列这些组件，并手动调整 GPU SMs 中用于通信与计算的比例。在这种重叠策略中，我们可以确保全对全通信和 PP 通信在执行过程中完全隐藏。

![]([https://conf01.birentech.com/download/attachments/202046727/image2025-2-12_11-24-55.png?version=1&modificationDate=1739330695000&api=v2](https://conf01.birentech.com/download/attachments/202046727/image2025-2-12_11-24-55.png?version=1&modificationDate=1739330695000&api=v2))

1. 开发高效的跨节点全通信内核，以充分利用 InfiniBand（IB）和 NVLink 的带宽。见 3.2.2 节

为了确保 DualPipe 具有足够的计算性能，我们定制了高效的跨节点全对全通信内核（包括分发和合并），以减少用于通信的流式多处理器（SM）数量。这些内核的实现与 MoE 门控算法和我们集群的网络拓扑共同设计。具体来说，在我们的集群中，跨节点的 GPU 通过 InfiniBand（IB）完全互连，而节点内的通信则通过 NVLink 处理。NVLink 提供 160 GB/s 的带宽，大约是 IB（50 GB/s）的 3.2 倍。为了有效利用 IB 和 NVLink 的不同带宽，我们限制每个令牌最多分发给 4 个节点，从而减少 IB 的流量。对于每个令牌，当路由决策完成后，它将首先通过 IB 传输到目标节点上具有相同节点内索引的 GPU。一旦到达目标节点，我们将确保它通过 NVLink 即时转发到托管其目标专家的特定 GPU，而不会被后续到达的令牌阻塞。通过这种方式，IB 和 NVLink 的通信完全重叠，每个令牌可以高效地选择每个节点平均 3.2 个专家，而不会因 NVLink 产生额外开销。这意味着，尽管 DeepSeek-V 3 在实践中仅选择 8 个路由专家，但它可以将这一数量扩展到最多 13 个专家（4 个节点 × 3.2 个专家/节点），同时保持相同的通信成本。总体而言，在这种通信策略下，仅需 20 个 SM 即可充分利用 IB 和 NVLink 的带宽。

具体来说，我们采用了**warp specialization**技术（Bauer et al., 2014），并将 20 个 SM 划分为 10 个通信通道。在分发过程中，（1）IB 发送、（2）IB 到 NVLink 转发以及（3）NVLink 接收分别由不同的 warp 处理。每个通信任务分配的 warp 数量会根据所有 SM 的实际工作负载动态调整。类似地，在合并过程中，（1）NVLink 发送、（2）NVLink 到 IB 转发和累加以及（3）IB 接收和累加也由动态调整的 warp 处理。此外，分发和合并内核与计算流重叠，因此我们还考虑了它们对其他 SM 计算内核的影响。具体来说，我们采用了定制的 PTX（并行线程执行）指令，并自动调整通信块大小，从而显著减少了 L 2 缓存的使用以及对其他 SM 的干扰。

**关键点总结**

1. **通信内核的定制**：

- 通过协同设计 MoE 门控算法和集群网络拓扑，优化跨节点全对全通信。
- 减少用于通信的 SMs 数量，提升计算性能。

1. **带宽优化**：

- 利用 NVLink 的高带宽（160 GB/s）和 IB 的中等带宽（50 GB/s）。
- 限制每个 token 最多分发到 4 个节点，减少 IB 流量。

1. **路由与转发机制**：

- token 通过 IB 传输到目标节点后，立即通过 NVLink 转发到目标专家 GPU。
- 确保通信过程中无阻塞，最大化带宽利用率。

1. **扩展性与成本控制**：

- 在保持通信成本不变的情况下，支持从 8 个路由专家扩展到最多 13 个专家。
- 仅需 20 个 SMs 即可充分利用 IB 和 NVLink 的带宽。

1. 精心优化了内存占用，没有使用 Tensor Parallel

为了减少训练期间的内存占用，我们采用了以下技术：

**RMSNorm 和 MLA 上投影的重计算**：我们在反向传播期间重新计算所有 RMSNorm 操作和 MLA 上投影，从而避免了持久存储它们的输出激活值。尽管这会带来轻微的计算开销，但该策略显著减少了存储激活值所需的内存。

**CPU 中的指数移动平均**：在训练过程中，我们保存模型参数的指数移动平均（EMA），以便在学习率衰减后对模型性能进行早期评估。EMA 参数存储在 CPU 内存中，并在每个训练步骤后异步更新。这种方法使我们能够在不增加额外内存或时间开销的情况下维护 EMA 参数。

**多令牌预测的共享嵌入和输出头**：通过 DualPipe 策略，我们将模型的最浅层（包括嵌入层）和最深层（包括输出头）部署在同一个流水线并行（PP）层级上。这种安排使得多令牌预测（MTP）模块与共享嵌入和输出头之间的参数和梯度能够物理共享，从而进一步优化内存使用
